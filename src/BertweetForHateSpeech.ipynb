{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BertweetForHateSpeech.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UGDxI5Fyb1Dd"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02m8sSA7Po69"
      },
      "source": [
        "**This code is for CSoNet-2021 paper \"Detecting Hate Speech Contents Using Embedding Models\".** \n",
        "\n",
        "**The resources of this paper are available at [here](https://github.com/duonghuuphuc/hate-speech-detection)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we5c0YwbaJie"
      },
      "source": [
        "# **Download Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjRwB3C6afvy",
        "outputId": "6004809d-676f-4c65-8dc9-936ab8d97a87"
      },
      "source": [
        "!mkdir datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘datasets’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BilV0lfwaUA0"
      },
      "source": [
        "## HASOC-2019"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWl2IJY9Hut5",
        "outputId": "f9680192-f7ca-491d-c431-1e88e8776c89"
      },
      "source": [
        "!mkdir ./datasets/HASOC2019\n",
        "!wget -O ./datasets/HASOC2019/english_dataset.tsv https://raw.githubusercontent.com/socialmediaie/HASOC2019/master/data/raw/training_data/english_dataset.tsv\n",
        "!wget -O ./datasets/HASOC2019/test_english_dataset.tsv https://raw.githubusercontent.com/socialmediaie/HASOC2019/master/data/raw/test_data_gold/english_data.tsv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./datasets/HASOC2019’: File exists\n",
            "--2021-07-09 03:12:22--  https://raw.githubusercontent.com/socialmediaie/HASOC2019/master/data/raw/training_data/english_dataset.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1223457 (1.2M) [text/plain]\n",
            "Saving to: ‘./datasets/HASOC2019/english_dataset.tsv’\n",
            "\n",
            "./datasets/HASOC201 100%[===================>]   1.17M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-07-09 03:12:22 (25.1 MB/s) - ‘./datasets/HASOC2019/english_dataset.tsv’ saved [1223457/1223457]\n",
            "\n",
            "--2021-07-09 03:12:22--  https://raw.githubusercontent.com/socialmediaie/HASOC2019/master/data/raw/test_data_gold/english_data.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231386 (226K) [text/plain]\n",
            "Saving to: ‘./datasets/HASOC2019/test_english_dataset.tsv’\n",
            "\n",
            "./datasets/HASOC201 100%[===================>] 225.96K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-07-09 03:12:23 (12.0 MB/s) - ‘./datasets/HASOC2019/test_english_dataset.tsv’ saved [231386/231386]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGDxI5Fyb1Dd"
      },
      "source": [
        "## Davidson-2017"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJrB2tEWb7vV",
        "outputId": "403dab07-b9d3-4811-a651-d23f814d9a45"
      },
      "source": [
        "!mkdir ./datasets/Davidson2017\n",
        "!wget -O ./datasets/Davidson2017/labeled_data.csv https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./datasets/Davidson2017’: File exists\n",
            "--2021-07-09 03:12:36--  https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2546446 (2.4M) [text/plain]\n",
            "Saving to: ‘./datasets/Davidson2017/labeled_data.csv’\n",
            "\n",
            "./datasets/Davidson 100%[===================>]   2.43M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-07-09 03:12:36 (33.8 MB/s) - ‘./datasets/Davidson2017/labeled_data.csv’ saved [2546446/2546446]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POhuWi_Cb8Kj"
      },
      "source": [
        "## HS2-2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc4TCBWucAkE",
        "outputId": "09ec7ae3-73fe-451b-e9e6-d63540c2d163"
      },
      "source": [
        "!mkdir ./datasets/HS2-2021"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./datasets/HS2-2021’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMKHCkgqcDia"
      },
      "source": [
        "# **Import Library & Package**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf2z01EYeCcu",
        "outputId": "1c94d5db-d5ae-4717-ce3f-a5166f7cbccc"
      },
      "source": [
        "!pip3 install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZwILzKOcQVf"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from tqdm import tqdm\n",
        "from transformers import  AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "\n",
        "import csv\n",
        "import copy\n",
        "import torch\n",
        "import time\n",
        "import logging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbJB5GVQ4KYv"
      },
      "source": [
        "logging.basicConfig(format='%(message)s', level=logging.INFO, filemode='a')\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzMjDwNfcPfP"
      },
      "source": [
        "# **Data Preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HRU1tkXg3Tk"
      },
      "source": [
        "def read_tsv(file_path, text_idx, class_idx, delimiter='\\t'):\n",
        "    samples = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        tsv_reader = csv.reader(f, delimiter=delimiter)\n",
        "        next(tsv_reader)\n",
        "        for row in tsv_reader:\n",
        "          samples.append((row[text_idx].strip(), row[class_idx].strip()))\n",
        "    return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54-l7sqccUhb"
      },
      "source": [
        "class BertweetDataset(Dataset):\n",
        "    def __init__(self, data_file_path, bert_tokenizer, label_map, text_idx, class_idx, delimiter='\\t', batch_size=32, max_length=128):\n",
        "        self.file_path = data_file_path\n",
        "        self.text_idx = text_idx\n",
        "        self.class_idx = class_idx\n",
        "        self.delimiter = delimiter\n",
        "        self.bz = batch_size\n",
        "        self.max_len = max_length\n",
        "        self.label_map = label_map\n",
        "        self.examples = []\n",
        "        self.create_examples(bert_tokenizer)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        example = self.examples[index]\n",
        "        token_ids = torch.tensor(example[0], dtype=torch.long)\n",
        "        token_type_ids = torch.tensor(example[1], dtype=torch.long)\n",
        "        attention_masks = torch.tensor(example[2], dtype=torch.long)\n",
        "        labels_id = torch.tensor(example[3], dtype=torch.long)\n",
        "        return token_ids, token_type_ids, attention_masks, labels_id\n",
        "\n",
        "    def create_examples(self, bert_tokenizer):\n",
        "        samples = read_tsv(self.file_path, self.text_idx, self.class_idx, self.delimiter)\n",
        "        sidx, eidx = 0, self.bz\n",
        "        pbar = tqdm(total=len(samples), position=0)\n",
        "        while sidx <= len(samples):\n",
        "            batch_samples = samples[sidx: eidx]\n",
        "            if len(batch_samples) == 0:\n",
        "                break\n",
        "            texts, labels = list(zip(*batch_samples))\n",
        "            label_ids = [self.label_map[label] for label in labels]\n",
        "            encoded_inputs = bert_tokenizer(text=texts, max_length=self.max_len, padding='max_length', truncation='longest_first')\n",
        "            encoded_inputs[\"label_ids\"] = label_ids\n",
        "            self.examples.extend(list(zip(*encoded_inputs.values())))\n",
        "            sidx += self.bz\n",
        "            eidx += self.bz\n",
        "            pbar.update(len(batch_samples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGXA_-D0fSg6"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0ZTDIAMfR_5"
      },
      "source": [
        "def build_model(model_name, num_labels, task_name, lr, weight_decay, device='cuda'):\n",
        "  model_config = AutoConfig.from_pretrained(model_name, num_labels=num_labels, finetuning_task=task_name)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, config=model_config)\n",
        "  no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "  optimizer_grouped_parameters = [\n",
        "      {\n",
        "          \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "          \"weight_decay\": weight_decay,\n",
        "      },\n",
        "      {\n",
        "          \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "          \"weight_decay\": 0.0,\n",
        "      },\n",
        "  ]\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "  model.to(device)\n",
        "  return model, optimizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDH34qDwj6d9"
      },
      "source": [
        "def train(model, optimizer, data_iter, device='cuda'):\n",
        "    model.train()\n",
        "    train_bar = tqdm(data_iter, total=len(data_iter), desc='\\tTRAIN:', position=0, leave=None)\n",
        "    train_loss = 0\n",
        "    train_preds, train_golds = [], []\n",
        "    start_time = time.time()\n",
        "    for batch in train_bar:\n",
        "        input_ids, token_type_ids, att_masks, label_ids = batch\n",
        "        if device == \"cuda\":\n",
        "            input_ids = input_ids.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "            att_masks = att_masks.to(device)\n",
        "            label_ids = label_ids.to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=att_masks, token_type_ids=token_type_ids, labels=label_ids)\n",
        "        outputs.loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss += outputs.loss.item()\n",
        "        train_preds += [y.argmax().item() for y in outputs.logits]\n",
        "        train_golds += label_ids.tolist()\n",
        "    acc_score, f1_macro, f1_weighted = caculate_score(train_golds, train_preds)\n",
        "    train_loss = train_loss / len(data_iter)\n",
        "    return model, train_loss, (acc_score, f1_macro, f1_weighted), get_total_time(start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqP5n0-ckTho"
      },
      "source": [
        "def eval(model, data_iter, device='cuda'):\n",
        "    model.eval()\n",
        "    eval_bar = tqdm(data_iter, total=len(data_iter), desc='\\tEVAL:', position=0, leave=None)\n",
        "    eval_loss = 0\n",
        "    eval_preds, eval_golds = [], []\n",
        "    start_time = time.time()\n",
        "    for batch in eval_bar:\n",
        "        input_ids, token_type_ids, att_masks, label_ids = batch\n",
        "        if device == \"cuda\":\n",
        "            input_ids = input_ids.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "            att_masks = att_masks.to(device)\n",
        "            label_ids = label_ids.to(device)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=att_masks, token_type_ids=token_type_ids, labels=label_ids)\n",
        "        eval_loss += outputs.loss.item()\n",
        "        eval_preds += [y.argmax().item() for y in outputs.logits]\n",
        "        eval_golds += label_ids.tolist()\n",
        "    acc_score, f1_macro, f1_weighted = caculate_score(eval_golds, eval_preds)\n",
        "    eval_loss = eval_loss / len(data_iter)\n",
        "    return eval_loss, (acc_score, f1_macro, f1_weighted), get_total_time(start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFleUnA--xxF"
      },
      "source": [
        "def parse_time(run_time):\n",
        "    hours, rem = divmod(run_time, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    return \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(minutes), seconds)\n",
        "\n",
        "def get_total_time(start_time):\n",
        "    end = time.time()\n",
        "    spended_time = end - start_time\n",
        "    return parse_time(spended_time)\n",
        "\n",
        "def caculate_score(actuals, predicts):\n",
        "    acc_score = metrics.accuracy_score(actuals, predicts)\n",
        "    f1_macro_score = metrics.f1_score(actuals, predicts, average=\"macro\")\n",
        "    f1_weighted_score = metrics.f1_score(actuals, predicts, average=\"weighted\")\n",
        "    return acc_score, f1_macro_score, f1_weighted_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stN9Ym7PgX82"
      },
      "source": [
        "# **Experiments**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQPQ0TD-hQKP"
      },
      "source": [
        "## HASOC-2019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWKqGcj4hS-Y"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTHJlQZeih5L"
      },
      "source": [
        "task_name = \"HASOC\"\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "# Dataset Options\n",
        "train_file_path = \"./datasets/HASOC2019/english_dataset.tsv\"\n",
        "test_file_path = \"./datasets/HASOC2019/test_english_dataset.tsv\"\n",
        "output_path = \"./outputs\"\n",
        "text_idx = 1\n",
        "class_idx = 2\n",
        "delimiter = \"\\t\"\n",
        "label_maps = {\"NOT\": 0, \"HOF\": 1}\n",
        "# Task Options\n",
        "num_exp = 5\n",
        "kfold = 5\n",
        "num_epochs = 10\n",
        "train_batch_size = 32\n",
        "test_batch_size = 16\n",
        "max_length = 128\n",
        "weight_decay = 0.0\n",
        "learning_rate = 0.00001\n",
        "early_stop = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NNxdEkDiHE0"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVDzQYzblEza",
        "outputId": "cf2a3994-4eb4-43de-b11d-6bbf4bf7aac2"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, normalization=True)\n",
        "train_dataset = BertweetDataset(train_file_path, tokenizer, label_maps, text_idx, class_idx, delimiter, batch_size=train_batch_size, max_length=max_length)\n",
        "test_dataset = BertweetDataset(test_file_path, tokenizer, label_maps, text_idx, class_idx, delimiter, batch_size=train_batch_size, max_length=max_length)\n",
        "test_iter = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "100%|██████████| 5852/5852 [00:02<00:00, 2195.22it/s]\n",
            "100%|██████████| 1153/1153 [00:00<00:00, 2292.13it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU0Tqzuml4rI",
        "outputId": "4b291e8b-2aa5-4467-df05-313803f64645"
      },
      "source": [
        "exp_scores = {\"acc\": [], \"f1_macro\": [], \"f1_weighted\": [], \"exp_time\": [], \"epoch_time\": [], \"fold_time\": []}\n",
        "for exp in range(num_exp):\n",
        "    exp_time = 0\n",
        "    kf = KFold(n_splits=kfold, shuffle=True)\n",
        "    avg_scores = {\"acc\": [], \"f1_macro\": [], \"f1_weighted\": []}\n",
        "    data_idxs = list(range(train_dataset.__len__()))\n",
        "    for idx, (train_idx, eval_idx) in enumerate(kf.split(data_idxs)):\n",
        "        best_epoch, best_loss, best_score = 0, float(\"inf\"), 0\n",
        "        model, optimizer = build_model(model_name, len(set(label_maps.values())), task_name, learning_rate, weight_decay)\n",
        "        best_model = copy.deepcopy(model)\n",
        "        train_iter = DataLoader(train_dataset, batch_size=train_batch_size, sampler=SubsetRandomSampler(train_idx))\n",
        "        eval_iter = DataLoader(train_dataset, batch_size=test_batch_size, sampler=SubsetRandomSampler(eval_idx))\n",
        "        fold_time = 0\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_time = time.time()\n",
        "            logger.info(f\"Exp: {exp+1} - Fold: {idx+1} - Epoch: {epoch+1}/{num_epochs}\")\n",
        "            model, train_loss, train_score, train_time = train(model, optimizer, train_iter)\n",
        "            epoch_time = time.time() - epoch_time\n",
        "            fold_time += epoch_time\n",
        "            exp_scores[\"epoch_time\"].append(epoch_time)\n",
        "            eval_loss, eval_score, eval_time = eval(model, eval_iter)\n",
        "            logger.info(f\"\\tEVAL  - Time: {eval_time}; AVG Loss: {eval_loss:.6f}; Accurancy: {eval_score[0]:.4f}; F1_maro: {eval_score[1]:.4f}; F1_weighted: {eval_score[2]:.4f}\")\n",
        "            if best_score <= eval_score[1]:\n",
        "              best_model = copy.deepcopy(model)\n",
        "              best_score = eval_score[1]\n",
        "              best_epoch = epoch\n",
        "            if best_loss >= eval_loss:\n",
        "              best_loss = eval_loss\n",
        "              counter = 0\n",
        "            else:\n",
        "              counter += 1\n",
        "            if counter >= early_stop:\n",
        "              break\n",
        "        exp_time += fold_time\n",
        "        exp_scores[\"fold_time\"].append(fold_time)\n",
        "        logger.info(f\"Test at epoch {best_epoch+1}:\")\n",
        "        test_loss, test_score, test_time = eval(best_model, test_iter)\n",
        "        logger.info(f\"\\tTEST  - Time: {test_time}; AVG Loss: {test_loss:.6f}; Accurancy: {test_score[0]:.4f}; F1_maro: {test_score[1]:.4f}; F1_weighted: {test_score[2]:.4f}\")\n",
        "        avg_scores[\"acc\"].append(test_score[0])\n",
        "        avg_scores[\"f1_macro\"].append(test_score[1])\n",
        "        avg_scores[\"f1_weighted\"].append(test_score[2])\n",
        "    logger.info(\"Summary:\")\n",
        "    fold_acc = (sum(avg_scores['acc'])/kfold)\n",
        "    fold_f1_macro = (sum(avg_scores['f1_macro'])/kfold)\n",
        "    fold_f1_weighted = (sum(avg_scores['f1_weighted'])/kfold)\n",
        "    logger.info(f\"\\tAccurancy: {fold_acc:.4f}\")\n",
        "    logger.info(f\"\\tF1 Macro: {fold_f1_macro:.4f}\")\n",
        "    logger.info(f\"\\tF1 Weighted: {fold_f1_weighted:.4f}\")\n",
        "    exp_scores[\"acc\"].append(fold_acc)\n",
        "    exp_scores[\"f1_macro\"].append(fold_f1_macro)\n",
        "    exp_scores[\"f1_weighted\"].append(fold_f1_weighted)\n",
        "    exp_scores[\"exp_time\"].append(exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 1 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.591959; Accurancy: 0.6806; F1_maro: 0.6631; F1_weighted: 0.6804\n",
            "Exp: 1 - Fold: 1 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.576154; Accurancy: 0.7165; F1_maro: 0.6706; F1_weighted: 0.6982\n",
            "Exp: 1 - Fold: 1 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.64it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.643748; Accurancy: 0.6516; F1_maro: 0.6486; F1_weighted: 0.6559\n",
            "Exp: 1 - Fold: 1 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.615970; Accurancy: 0.6977; F1_maro: 0.6627; F1_weighted: 0.6871\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.62it/s]\n",
            "\tTEST  - Time: 00:00:04.68; AVG Loss: 0.409518; Accurancy: 0.8448; F1_maro: 0.7799; F1_weighted: 0.8397\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 2 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.586849; Accurancy: 0.6687; F1_maro: 0.6435; F1_weighted: 0.6682\n",
            "Exp: 1 - Fold: 2 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.653752; Accurancy: 0.6183; F1_maro: 0.6182; F1_weighted: 0.6193\n",
            "Exp: 1 - Fold: 2 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.587237; Accurancy: 0.6917; F1_maro: 0.6757; F1_weighted: 0.6945\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.62it/s]\n",
            "\tTEST  - Time: 00:00:04.68; AVG Loss: 0.419419; Accurancy: 0.8196; F1_maro: 0.7772; F1_weighted: 0.8258\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 3 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.66it/s]\n",
            "\tEVAL  - Time: 00:00:04.73; AVG Loss: 0.628752; Accurancy: 0.6496; F1_maro: 0.6138; F1_weighted: 0.6387\n",
            "Exp: 1 - Fold: 3 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.56it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.634736; Accurancy: 0.6513; F1_maro: 0.6428; F1_weighted: 0.6545\n",
            "Exp: 1 - Fold: 3 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.66it/s]\n",
            "\tEVAL  - Time: 00:00:04.73; AVG Loss: 0.658465; Accurancy: 0.6085; F1_maro: 0.4429; F1_weighted: 0.5073\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.63it/s]\n",
            "\tTEST  - Time: 00:00:04.68; AVG Loss: 0.486299; Accurancy: 0.7944; F1_maro: 0.7449; F1_weighted: 0.8012\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 4 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.578908; Accurancy: 0.6949; F1_maro: 0.6768; F1_weighted: 0.6957\n",
            "Exp: 1 - Fold: 4 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.70it/s]\n",
            "\tEVAL  - Time: 00:00:04.72; AVG Loss: 0.572103; Accurancy: 0.7017; F1_maro: 0.6894; F1_weighted: 0.7047\n",
            "Exp: 1 - Fold: 4 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.64it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.556473; Accurancy: 0.7120; F1_maro: 0.6924; F1_weighted: 0.7116\n",
            "Exp: 1 - Fold: 4 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.608248; Accurancy: 0.7009; F1_maro: 0.6816; F1_weighted: 0.7010\n",
            "Exp: 1 - Fold: 4 - Epoch: 5/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.685972; Accurancy: 0.6846; F1_maro: 0.6744; F1_weighted: 0.6887\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.65it/s]\n",
            "\tTEST  - Time: 00:00:04.67; AVG Loss: 0.443731; Accurancy: 0.8101; F1_maro: 0.7694; F1_weighted: 0.8179\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 5 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.597035; Accurancy: 0.6761; F1_maro: 0.6263; F1_weighted: 0.6524\n",
            "Exp: 1 - Fold: 5 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.615457; Accurancy: 0.6684; F1_maro: 0.6658; F1_weighted: 0.6714\n",
            "Exp: 1 - Fold: 5 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.603383; Accurancy: 0.6795; F1_maro: 0.6537; F1_weighted: 0.6718\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.66it/s]\n",
            "\tTEST  - Time: 00:00:04.67; AVG Loss: 0.485124; Accurancy: 0.7641; F1_maro: 0.7338; F1_weighted: 0.7787\n",
            "Summary:\n",
            "\tAccurancy: 0.8066\n",
            "\tF1 Macro: 0.7610\n",
            "\tF1 Weighted: 0.8127\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 1 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.64it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.588592; Accurancy: 0.6849; F1_maro: 0.6685; F1_weighted: 0.6846\n",
            "Exp: 2 - Fold: 1 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.594765; Accurancy: 0.6755; F1_maro: 0.6668; F1_weighted: 0.6785\n",
            "Exp: 2 - Fold: 1 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.63it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.592796; Accurancy: 0.6977; F1_maro: 0.6789; F1_weighted: 0.6958\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.61it/s]\n",
            "\tTEST  - Time: 00:00:04.68; AVG Loss: 0.421600; Accurancy: 0.8274; F1_maro: 0.7785; F1_weighted: 0.8306\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 2 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.66it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.584424; Accurancy: 0.6926; F1_maro: 0.6579; F1_weighted: 0.6838\n",
            "Exp: 2 - Fold: 2 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.568400; Accurancy: 0.7165; F1_maro: 0.6833; F1_weighted: 0.7077\n",
            "Exp: 2 - Fold: 2 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.559078; Accurancy: 0.7182; F1_maro: 0.6856; F1_weighted: 0.7097\n",
            "Exp: 2 - Fold: 2 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.62it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.588776; Accurancy: 0.7199; F1_maro: 0.6943; F1_weighted: 0.7154\n",
            "Exp: 2 - Fold: 2 - Epoch: 5/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.635419; Accurancy: 0.6917; F1_maro: 0.6501; F1_weighted: 0.6788\n",
            "Test at epoch 4:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.65it/s]\n",
            "\tTEST  - Time: 00:00:04.67; AVG Loss: 0.402666; Accurancy: 0.8326; F1_maro: 0.7809; F1_weighted: 0.8342\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 3 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.64it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.615812; Accurancy: 0.6786; F1_maro: 0.5941; F1_weighted: 0.6387\n",
            "Exp: 2 - Fold: 3 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.64it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.589458; Accurancy: 0.6795; F1_maro: 0.6645; F1_weighted: 0.6816\n",
            "Exp: 2 - Fold: 3 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.64it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.589340; Accurancy: 0.6991; F1_maro: 0.6738; F1_weighted: 0.6957\n",
            "Exp: 2 - Fold: 3 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.589436; Accurancy: 0.6923; F1_maro: 0.6639; F1_weighted: 0.6875\n",
            "Exp: 2 - Fold: 3 - Epoch: 5/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.63it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.728584; Accurancy: 0.6462; F1_maro: 0.6428; F1_weighted: 0.6512\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.63it/s]\n",
            "\tTEST  - Time: 00:00:04.68; AVG Loss: 0.392916; Accurancy: 0.8387; F1_maro: 0.7886; F1_weighted: 0.8401\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 4 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.63it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.610307; Accurancy: 0.6675; F1_maro: 0.6101; F1_weighted: 0.6416\n",
            "Exp: 2 - Fold: 4 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.601794; Accurancy: 0.6906; F1_maro: 0.6528; F1_weighted: 0.6769\n",
            "Exp: 2 - Fold: 4 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.68it/s]\n",
            "\tEVAL  - Time: 00:00:04.73; AVG Loss: 0.619485; Accurancy: 0.6974; F1_maro: 0.6546; F1_weighted: 0.6802\n",
            "Exp: 2 - Fold: 4 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.65it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.642690; Accurancy: 0.6812; F1_maro: 0.6497; F1_weighted: 0.6718\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.63it/s]\n",
            "\tTEST  - Time: 00:00:04.68; AVG Loss: 0.397637; Accurancy: 0.8448; F1_maro: 0.7769; F1_weighted: 0.8385\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 5 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.58it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.587207; Accurancy: 0.6863; F1_maro: 0.6299; F1_weighted: 0.6630\n",
            "Exp: 2 - Fold: 5 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.63it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.559358; Accurancy: 0.7026; F1_maro: 0.6788; F1_weighted: 0.6988\n",
            "Exp: 2 - Fold: 5 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.588658; Accurancy: 0.7060; F1_maro: 0.6713; F1_weighted: 0.6958\n",
            "Exp: 2 - Fold: 5 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.64it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.607122; Accurancy: 0.7111; F1_maro: 0.6737; F1_weighted: 0.6990\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.60it/s]\n",
            "\tTEST  - Time: 00:00:04.69; AVG Loss: 0.444311; Accurancy: 0.8387; F1_maro: 0.7882; F1_weighted: 0.8399\n",
            "Summary:\n",
            "\tAccurancy: 0.8364\n",
            "\tF1 Macro: 0.7826\n",
            "\tF1 Weighted: 0.8366\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 3 - Fold: 1 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.63it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.593366; Accurancy: 0.6763; F1_maro: 0.6629; F1_weighted: 0.6769\n",
            "Exp: 3 - Fold: 1 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.579435; Accurancy: 0.6951; F1_maro: 0.6803; F1_weighted: 0.6946\n",
            "Exp: 3 - Fold: 1 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.63it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.628250; Accurancy: 0.6678; F1_maro: 0.6659; F1_weighted: 0.6711\n",
            "Exp: 3 - Fold: 1 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.64it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.665039; Accurancy: 0.6482; F1_maro: 0.6469; F1_weighted: 0.6513\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.66it/s]\n",
            "\tTEST  - Time: 00:00:04.67; AVG Loss: 0.462173; Accurancy: 0.8213; F1_maro: 0.7742; F1_weighted: 0.8258\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 3 - Fold: 2 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.597030; Accurancy: 0.6815; F1_maro: 0.6370; F1_weighted: 0.6629\n",
            "Exp: 3 - Fold: 2 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.56it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.576327; Accurancy: 0.6960; F1_maro: 0.6551; F1_weighted: 0.6794\n",
            "Exp: 3 - Fold: 2 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.58it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.583346; Accurancy: 0.6943; F1_maro: 0.6720; F1_weighted: 0.6895\n",
            "Exp: 3 - Fold: 2 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.49it/s]\n",
            "\tEVAL  - Time: 00:00:04.79; AVG Loss: 0.615455; Accurancy: 0.6926; F1_maro: 0.6689; F1_weighted: 0.6870\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.62it/s]\n",
            "\tTEST  - Time: 00:00:04.68; AVG Loss: 0.411576; Accurancy: 0.8343; F1_maro: 0.7756; F1_weighted: 0.8331\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 3 - Fold: 3 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.56it/s]\n",
            "\tEVAL  - Time: 00:00:04.77; AVG Loss: 0.593240; Accurancy: 0.6915; F1_maro: 0.6299; F1_weighted: 0.6697\n",
            "Exp: 3 - Fold: 3 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.575112; Accurancy: 0.7017; F1_maro: 0.6818; F1_weighted: 0.7027\n",
            "Exp: 3 - Fold: 3 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.568421; Accurancy: 0.6906; F1_maro: 0.6594; F1_weighted: 0.6865\n",
            "Exp: 3 - Fold: 3 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.58it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.609829; Accurancy: 0.7060; F1_maro: 0.6799; F1_weighted: 0.7040\n",
            "Exp: 3 - Fold: 3 - Epoch: 5/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.63it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.718658; Accurancy: 0.6726; F1_maro: 0.6629; F1_weighted: 0.6780\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.62it/s]\n",
            "\tTEST  - Time: 00:00:04.68; AVG Loss: 0.471401; Accurancy: 0.8352; F1_maro: 0.7878; F1_weighted: 0.8380\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 3 - Fold: 4 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.56it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.624945; Accurancy: 0.6385; F1_maro: 0.6251; F1_weighted: 0.6405\n",
            "Exp: 3 - Fold: 4 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.634537; Accurancy: 0.6761; F1_maro: 0.6241; F1_weighted: 0.6544\n",
            "Exp: 3 - Fold: 4 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.628124; Accurancy: 0.6769; F1_maro: 0.6471; F1_weighted: 0.6694\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.54it/s]\n",
            "\tTEST  - Time: 00:00:04.70; AVG Loss: 0.388717; Accurancy: 0.8361; F1_maro: 0.7873; F1_weighted: 0.8383\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 3 - Fold: 5 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.56it/s]\n",
            "\tEVAL  - Time: 00:00:04.77; AVG Loss: 0.594416; Accurancy: 0.6863; F1_maro: 0.6628; F1_weighted: 0.6846\n",
            "Exp: 3 - Fold: 5 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.580295; Accurancy: 0.7085; F1_maro: 0.6822; F1_weighted: 0.7046\n",
            "Exp: 3 - Fold: 5 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.56it/s]\n",
            "\tEVAL  - Time: 00:00:04.77; AVG Loss: 0.585726; Accurancy: 0.7060; F1_maro: 0.6767; F1_weighted: 0.7005\n",
            "Exp: 3 - Fold: 5 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.606958; Accurancy: 0.7128; F1_maro: 0.6757; F1_weighted: 0.7025\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.60it/s]\n",
            "\tTEST  - Time: 00:00:04.69; AVG Loss: 0.444976; Accurancy: 0.8395; F1_maro: 0.7940; F1_weighted: 0.8425\n",
            "Summary:\n",
            "\tAccurancy: 0.8333\n",
            "\tF1 Macro: 0.7838\n",
            "\tF1 Weighted: 0.8355\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 4 - Fold: 1 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.58it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.595461; Accurancy: 0.6661; F1_maro: 0.6532; F1_weighted: 0.6659\n",
            "Exp: 4 - Fold: 1 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.58it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.590517; Accurancy: 0.6857; F1_maro: 0.6490; F1_weighted: 0.6706\n",
            "Exp: 4 - Fold: 1 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.625323; Accurancy: 0.6823; F1_maro: 0.6146; F1_weighted: 0.6453\n",
            "Exp: 4 - Fold: 1 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.658043; Accurancy: 0.6687; F1_maro: 0.6626; F1_weighted: 0.6712\n",
            "Test at epoch 4:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.60it/s]\n",
            "\tTEST  - Time: 00:00:04.69; AVG Loss: 0.469037; Accurancy: 0.7780; F1_maro: 0.7438; F1_weighted: 0.7906\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 4 - Fold: 2 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.594746; Accurancy: 0.6687; F1_maro: 0.6475; F1_weighted: 0.6651\n",
            "Exp: 4 - Fold: 2 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.562865; Accurancy: 0.7079; F1_maro: 0.6746; F1_weighted: 0.6958\n",
            "Exp: 4 - Fold: 2 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.56it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.578554; Accurancy: 0.7088; F1_maro: 0.6833; F1_weighted: 0.7017\n",
            "Exp: 4 - Fold: 2 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.54it/s]\n",
            "\tEVAL  - Time: 00:00:04.77; AVG Loss: 0.651613; Accurancy: 0.6627; F1_maro: 0.6569; F1_weighted: 0.6660\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.59it/s]\n",
            "\tTEST  - Time: 00:00:04.69; AVG Loss: 0.410197; Accurancy: 0.8361; F1_maro: 0.7821; F1_weighted: 0.8364\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 4 - Fold: 3 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.56it/s]\n",
            "\tEVAL  - Time: 00:00:04.77; AVG Loss: 0.578067; Accurancy: 0.6906; F1_maro: 0.6631; F1_weighted: 0.6903\n",
            "Exp: 4 - Fold: 3 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.555274; Accurancy: 0.7145; F1_maro: 0.6784; F1_weighted: 0.7088\n",
            "Exp: 4 - Fold: 3 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.580072; Accurancy: 0.6974; F1_maro: 0.6807; F1_weighted: 0.7013\n",
            "Exp: 4 - Fold: 3 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.609379; Accurancy: 0.6923; F1_maro: 0.6778; F1_weighted: 0.6971\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.58it/s]\n",
            "\tTEST  - Time: 00:00:04.69; AVG Loss: 0.460636; Accurancy: 0.8031; F1_maro: 0.7681; F1_weighted: 0.8132\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 4 - Fold: 4 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.58it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.605222; Accurancy: 0.6556; F1_maro: 0.6378; F1_weighted: 0.6585\n",
            "Exp: 4 - Fold: 4 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.592834; Accurancy: 0.6761; F1_maro: 0.6528; F1_weighted: 0.6760\n",
            "Exp: 4 - Fold: 4 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.614305; Accurancy: 0.6795; F1_maro: 0.6558; F1_weighted: 0.6791\n",
            "Exp: 4 - Fold: 4 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.621318; Accurancy: 0.6855; F1_maro: 0.6664; F1_weighted: 0.6870\n",
            "Test at epoch 4:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.51it/s]\n",
            "\tTEST  - Time: 00:00:04.71; AVG Loss: 0.416217; Accurancy: 0.8127; F1_maro: 0.7682; F1_weighted: 0.8190\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 4 - Fold: 5 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.603198; Accurancy: 0.6684; F1_maro: 0.6612; F1_weighted: 0.6712\n",
            "Exp: 4 - Fold: 5 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.574211; Accurancy: 0.7179; F1_maro: 0.7031; F1_weighted: 0.7165\n",
            "Exp: 4 - Fold: 5 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.59it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.593127; Accurancy: 0.6897; F1_maro: 0.6801; F1_weighted: 0.6913\n",
            "Exp: 4 - Fold: 5 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.615298; Accurancy: 0.7085; F1_maro: 0.6882; F1_weighted: 0.7043\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.61it/s]\n",
            "\tTEST  - Time: 00:00:04.68; AVG Loss: 0.434814; Accurancy: 0.8352; F1_maro: 0.7882; F1_weighted: 0.8382\n",
            "Summary:\n",
            "\tAccurancy: 0.8130\n",
            "\tF1 Macro: 0.7701\n",
            "\tF1 Weighted: 0.8195\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 5 - Fold: 1 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.58it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.596948; Accurancy: 0.6849; F1_maro: 0.6052; F1_weighted: 0.6499\n",
            "Exp: 5 - Fold: 1 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.582756; Accurancy: 0.6917; F1_maro: 0.6630; F1_weighted: 0.6878\n",
            "Exp: 5 - Fold: 1 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.58it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.594206; Accurancy: 0.6977; F1_maro: 0.6714; F1_weighted: 0.6948\n",
            "Exp: 5 - Fold: 1 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.54it/s]\n",
            "\tEVAL  - Time: 00:00:04.77; AVG Loss: 0.667382; Accurancy: 0.6866; F1_maro: 0.6555; F1_weighted: 0.6816\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.60it/s]\n",
            "\tTEST  - Time: 00:00:04.69; AVG Loss: 0.402930; Accurancy: 0.8413; F1_maro: 0.7958; F1_weighted: 0.8440\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 5 - Fold: 2 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.63it/s]\n",
            "\tEVAL  - Time: 00:00:04.74; AVG Loss: 0.598235; Accurancy: 0.6883; F1_maro: 0.6607; F1_weighted: 0.6841\n",
            "Exp: 5 - Fold: 2 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.58it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.598438; Accurancy: 0.6968; F1_maro: 0.6231; F1_weighted: 0.6634\n",
            "Exp: 5 - Fold: 2 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.55it/s]\n",
            "\tEVAL  - Time: 00:00:04.77; AVG Loss: 0.580299; Accurancy: 0.7037; F1_maro: 0.6594; F1_weighted: 0.6891\n",
            "Exp: 5 - Fold: 2 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.58it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.628251; Accurancy: 0.6712; F1_maro: 0.6587; F1_weighted: 0.6745\n",
            "Exp: 5 - Fold: 2 - Epoch: 5/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.654126; Accurancy: 0.6857; F1_maro: 0.6485; F1_weighted: 0.6762\n",
            "Test at epoch 1:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.58it/s]\n",
            "\tTEST  - Time: 00:00:04.69; AVG Loss: 0.472340; Accurancy: 0.8101; F1_maro: 0.7247; F1_weighted: 0.8014\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 5 - Fold: 3 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.599507; Accurancy: 0.6684; F1_maro: 0.6567; F1_weighted: 0.6701\n",
            "Exp: 5 - Fold: 3 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.578970; Accurancy: 0.7000; F1_maro: 0.6657; F1_weighted: 0.6884\n",
            "Exp: 5 - Fold: 3 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.598741; Accurancy: 0.6718; F1_maro: 0.6547; F1_weighted: 0.6710\n",
            "Exp: 5 - Fold: 3 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.718187; Accurancy: 0.6615; F1_maro: 0.6546; F1_weighted: 0.6650\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.58it/s]\n",
            "\tTEST  - Time: 00:00:04.69; AVG Loss: 0.409486; Accurancy: 0.8448; F1_maro: 0.7737; F1_weighted: 0.8372\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 5 - Fold: 4 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.622226; Accurancy: 0.6590; F1_maro: 0.5738; F1_weighted: 0.6113\n",
            "Exp: 5 - Fold: 4 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.584676; Accurancy: 0.6915; F1_maro: 0.6693; F1_weighted: 0.6861\n",
            "Exp: 5 - Fold: 4 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.54it/s]\n",
            "\tEVAL  - Time: 00:00:04.77; AVG Loss: 0.605315; Accurancy: 0.6812; F1_maro: 0.6709; F1_weighted: 0.6823\n",
            "Exp: 5 - Fold: 4 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.61it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.602238; Accurancy: 0.6752; F1_maro: 0.6561; F1_weighted: 0.6720\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.57it/s]\n",
            "\tTEST  - Time: 00:00:04.70; AVG Loss: 0.448109; Accurancy: 0.8109; F1_maro: 0.7753; F1_weighted: 0.8201\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 5 - Fold: 5 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.58it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.577939; Accurancy: 0.7077; F1_maro: 0.6611; F1_weighted: 0.6905\n",
            "Exp: 5 - Fold: 5 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.56it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.557966; Accurancy: 0.7188; F1_maro: 0.6940; F1_weighted: 0.7144\n",
            "Exp: 5 - Fold: 5 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.60it/s]\n",
            "\tEVAL  - Time: 00:00:04.75; AVG Loss: 0.568410; Accurancy: 0.7171; F1_maro: 0.6947; F1_weighted: 0.7141\n",
            "Exp: 5 - Fold: 5 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 147/147 [00:56<00:00,  2.62it/s]\n",
            "\tEVAL:: 100%|██████████| 74/74 [00:04<00:00, 15.57it/s]\n",
            "\tEVAL  - Time: 00:00:04.76; AVG Loss: 0.596327; Accurancy: 0.7299; F1_maro: 0.6925; F1_weighted: 0.7176\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 73/73 [00:04<00:00, 15.54it/s]\n",
            "\tTEST  - Time: 00:00:04.70; AVG Loss: 0.401967; Accurancy: 0.8361; F1_maro: 0.7905; F1_weighted: 0.8394\n",
            "Summary:\n",
            "\tAccurancy: 0.8286\n",
            "\tF1 Macro: 0.7720\n",
            "\tF1 Weighted: 0.8284\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG2Mo-odiHV_"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDG0endqDm-0",
        "outputId": "60b43134-7d3c-4001-bb6d-43b8c11d8ccf"
      },
      "source": [
        "logger.info(f\"Accurancy: {exp_scores['acc']}\")\n",
        "logger.info(f\"AVG Accurancy: {(sum(exp_scores['acc'])/num_exp):.4f} - MAX: {max(exp_scores['acc']):.4f} - MIN: {min(exp_scores['acc']):.4f}\")\n",
        "logger.info(f\"F1 Macro: {exp_scores['f1_macro']}\")\n",
        "logger.info(f\"AVG F1 Macro: {(sum(exp_scores['f1_macro'])/num_exp):.4f} - MAX: {max(exp_scores['f1_macro']):.4f} - MIN: {min(exp_scores['f1_macro']):.4f}\")\n",
        "logger.info(f\"F1 Weighted: {exp_scores['f1_weighted']}\")\n",
        "logger.info(f\"AVG F1 Weighted: {(sum(exp_scores['f1_weighted'])/num_exp):.4f} - MAX: {max(exp_scores['f1_weighted']):.4f} - MIN: {min(exp_scores['f1_weighted']):.4f}\")\n",
        "logger.info(f\"Exp time: {exp_scores['exp_time']}\")\n",
        "logger.info(f\"AVG Exp time: {parse_time(sum(exp_scores['exp_time'])/num_exp)}\")\n",
        "logger.info(f\"Fold time: {exp_scores['fold_time']}\")\n",
        "logger.info(f\"AVG Fold time: {parse_time(sum(exp_scores['fold_time'])/len(exp_scores['fold_time']))}\")\n",
        "logger.info(f\"Epoch time: {exp_scores['epoch_time']}\")\n",
        "logger.info(f\"AVG Epoch time: {parse_time(sum(exp_scores['epoch_time'])/len(exp_scores['epoch_time']))}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accurancy: [0.8065915004336514, 0.83642671292281, 0.8333044232437121, 0.813009540329575, 0.8286209887250651]\n",
            "AVG Accurancy: 0.8236 - MAX: 0.8364 - MIN: 0.8066\n",
            "F1 Macro: [0.7610441573542819, 0.7826131453095382, 0.7837822762730859, 0.770088038643431, 0.7720094081083645]\n",
            "AVG F1 Macro: 0.7739 - MAX: 0.7838 - MIN: 0.7610\n",
            "F1 Weighted: [0.8126585132082272, 0.8366496424531341, 0.8355268213513305, 0.8194694700196188, 0.8284189288217293]\n",
            "AVG F1 Weighted: 0.8265 - MAX: 0.8366 - MIN: 0.8127\n",
            "Exp time: [1010.7412478923798, 1178.764492034912, 1123.4087042808533, 1124.2814745903015, 1180.3996152877808]\n",
            "AVG Exp time: 00:18:43.52\n",
            "Fold time: [224.52934098243713, 168.39970707893372, 168.44618725776672, 280.9198799133301, 168.4461326599121, 168.36570024490356, 280.6077060699463, 280.6714689731598, 224.54959201812744, 224.57002472877502, 224.54648089408875, 224.58594942092896, 280.9306650161743, 168.5754039287567, 224.77020502090454, 224.9931833744049, 224.6971890926361, 224.7620770931244, 224.87699222564697, 224.95203280448914, 224.85610246658325, 281.09601736068726, 224.795592546463, 224.79679656028748, 224.85510635375977]\n",
            "AVG Fold time: 00:03:44.70\n",
            "Epoch time: [56.1363263130188, 56.161051988601685, 56.11514949798584, 56.11681318283081, 56.16212725639343, 56.09921979904175, 56.138360023498535, 56.14949989318848, 56.16190218925476, 56.134785175323486, 56.112425327301025, 56.134660482406616, 56.37554931640625, 56.15631365776062, 56.140931129455566, 56.195393800735474, 56.137531042099, 56.11320781707764, 56.1007559299469, 56.129233598709106, 56.13571071624756, 56.14202618598938, 56.093058586120605, 56.10622310638428, 56.12147831916809, 56.144919872283936, 56.13250732421875, 56.15128469467163, 56.11979961395264, 56.12729358673096, 56.140583753585815, 56.13045787811279, 56.15218925476074, 56.13527512550354, 56.131669759750366, 56.140201568603516, 56.15990591049194, 56.120882749557495, 56.14903450012207, 56.13706731796265, 56.15870761871338, 56.130226612091064, 56.120479345321655, 56.13478708267212, 56.11363124847412, 56.199854373931885, 56.13767671585083, 56.13365387916565, 56.20142912864685, 56.16297149658203, 56.19717860221863, 56.23543190956116, 56.23947858810425, 56.17638158798218, 56.15954375267029, 56.16610097885132, 56.219980001449585, 56.1726598739624, 56.211464166641235, 56.17000198364258, 56.203407526016235, 56.42373871803284, 56.19603514671326, 56.16029071807861, 56.16859459877014, 56.17074370384216, 56.19756007194519, 56.20699167251587, 56.2203893661499, 56.16259503364563, 56.17210102081299, 56.209200620651245, 56.17274880409241, 56.246315240859985, 56.248727560043335, 56.31011605262756, 56.19066905975342, 56.216469526290894, 56.23477816581726, 56.24579644203186, 56.18745160102844, 56.2107298374176, 56.21212458610535, 56.208845376968384, 56.12004494667053, 56.181801319122314, 56.13963770866394, 56.445688009262085, 56.18621802330017, 56.20041465759277, 56.20191717147827, 56.2070426940918, 56.14824175834656, 56.22051191329956, 56.24699378013611, 56.18104910850525, 56.20619440078735, 56.21638298034668, 56.284310817718506, 56.14821815490723]\n",
            "AVG Epoch time: 00:00:56.18\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHzVGgbqlivc"
      },
      "source": [
        "## Davidson-2017"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPfXtWcHlozb"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JjIeTbmlq_7"
      },
      "source": [
        "task_name = \"Davidson\"\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "# Dataset Options\n",
        "train_file_path = \"./datasets/Davidson2017/labeled_data.csv\"\n",
        "output_path = \"./outputs\"\n",
        "text_idx = 6\n",
        "class_idx = 5\n",
        "delimiter = \",\"\n",
        "label_maps = {\"0\": 1, \"1\": 1, \"2\": 0}\n",
        "test_split = 0.1\n",
        "# Task Options\n",
        "num_exp = 5\n",
        "kfold = 5\n",
        "num_epochs = 10\n",
        "train_batch_size = 32\n",
        "test_batch_size = 16\n",
        "max_length = 128\n",
        "weight_decay = 0.0\n",
        "learning_rate = 0.00001\n",
        "early_stop = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Yus6qUnF_C"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oZM85fenPQe",
        "outputId": "d6655d58-d770-4419-ed28-1a2f65594b3b"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, normalization=True)\n",
        "train_dataset = BertweetDataset(train_file_path, tokenizer, label_maps, text_idx, class_idx, delimiter, batch_size=train_batch_size, max_length=max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "100%|██████████| 24783/24783 [00:06<00:00, 3998.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-mGv-tznY68",
        "outputId": "78a0c068-a2cc-47a4-9a0c-7d717541c072"
      },
      "source": [
        "exp_scores = {\"acc\": [], \"f1_macro\": [], \"f1_weighted\": [], \"exp_time\": [], \"epoch_time\": [], \"fold_time\": []}\n",
        "for exp in range(num_exp):\n",
        "    exp_time = 0\n",
        "    kf = KFold(n_splits=kfold, shuffle=True)\n",
        "    avg_scores = {\"acc\": [], \"f1_macro\": [], \"f1_weighted\": []}\n",
        "    data_idxs = list(range(train_dataset.__len__()))\n",
        "    train_idx, test_idxs = train_test_split(data_idxs, test_size=test_split, shuffle=True)\n",
        "    test_iter = DataLoader(train_dataset, batch_size=test_batch_size, sampler=SubsetRandomSampler(test_idxs))\n",
        "    for idx, (train_idx, eval_idx) in enumerate(kf.split(data_idxs)):\n",
        "        best_epoch, best_loss, best_score = 0, float(\"inf\"), 0\n",
        "        model, optimizer = build_model(model_name, len(set(label_maps.values())), task_name, learning_rate, weight_decay)\n",
        "        best_model = copy.deepcopy(model)\n",
        "        train_iter = DataLoader(train_dataset, batch_size=train_batch_size, sampler=SubsetRandomSampler(train_idx))\n",
        "        eval_iter = DataLoader(train_dataset, batch_size=test_batch_size, sampler=SubsetRandomSampler(eval_idx))\n",
        "        fold_time = 0\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_time = time.time()\n",
        "            logger.info(f\"Exp: {exp+1} - Fold: {idx+1} - Epoch: {epoch+1}/{num_epochs}\")\n",
        "            model, train_loss, train_score, train_time = train(model, optimizer, train_iter)\n",
        "            epoch_time = time.time() - epoch_time\n",
        "            fold_time += epoch_time\n",
        "            exp_scores[\"epoch_time\"].append(epoch_time)\n",
        "            eval_loss, eval_score, eval_time = eval(model, eval_iter)\n",
        "            logger.info(f\"\\tEVAL  - Time: {eval_time}; AVG Loss: {eval_loss:.6f}; Accurancy: {eval_score[0]:.4f}; F1_maro: {eval_score[1]:.4f}; F1_weighted: {eval_score[2]:.4f}\")\n",
        "            if best_score <= eval_score[1]:\n",
        "              best_model = copy.deepcopy(model)\n",
        "              best_score = eval_score[1]\n",
        "              best_epoch = epoch\n",
        "            if best_loss >= eval_loss:\n",
        "              best_loss = eval_loss\n",
        "              counter = 0\n",
        "            else:\n",
        "              counter += 1\n",
        "            if counter >= early_stop:\n",
        "              break\n",
        "        exp_time += fold_time\n",
        "        exp_scores[\"fold_time\"].append(fold_time)\n",
        "        logger.info(f\"Test at epoch {best_epoch+1}:\")\n",
        "        test_loss, test_score, test_time = eval(best_model, test_iter)\n",
        "        logger.info(f\"\\tTEST  - Time: {test_time}; AVG Loss: {test_loss:.6f}; Accurancy: {test_score[0]:.4f}; F1_maro: {test_score[1]:.4f}; F1_weighted: {test_score[2]:.4f}\")\n",
        "        avg_scores[\"acc\"].append(test_score[0])\n",
        "        avg_scores[\"f1_macro\"].append(test_score[1])\n",
        "        avg_scores[\"f1_weighted\"].append(test_score[2])\n",
        "    logger.info(\"Summary:\")\n",
        "    fold_acc = (sum(avg_scores['acc'])/kfold)\n",
        "    fold_f1_macro = (sum(avg_scores['f1_macro'])/kfold)\n",
        "    fold_f1_weighted = (sum(avg_scores['f1_weighted'])/kfold)\n",
        "    logger.info(f\"\\tAccurancy: {avg_scores['acc']}\")\n",
        "    logger.info(f\"\\tAVG Accurancy: {fold_acc:.4f} - MAX: {max(avg_scores['acc']):.4f} - MIN: {min(avg_scores['acc']):.4f}\")\n",
        "    logger.info(f\"\\tF1 Macro: {avg_scores['f1_macro']}\")\n",
        "    logger.info(f\"\\tAVG F1 Macro: {fold_f1_macro:.4f} - MAX: {max(avg_scores['f1_macro']):.4f} - MIN: {min(avg_scores['f1_macro']):.4f}\")\n",
        "    logger.info(f\"\\tF1 Weighted: {avg_scores['f1_weighted']}\")\n",
        "    logger.info(f\"\\t AVG F1 Weighted: {(sum(avg_scores['f1_weighted'])/num_exp):.4f} - MAX: {max(avg_scores['f1_weighted']):.4f} - MIN: {min(avg_scores['f1_weighted']):.4f}\")\n",
        "    exp_scores[\"acc\"].append(fold_acc)\n",
        "    exp_scores[\"f1_macro\"].append(fold_f1_macro)\n",
        "    exp_scores[\"f1_weighted\"].append(fold_f1_weighted)\n",
        "    exp_scores[\"exp_time\"].append(exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 1 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.32; AVG Loss: 0.083703; Accurancy: 0.9703; F1_maro: 0.9491; F1_weighted: 0.9703\n",
            "Exp: 1 - Fold: 1 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.25it/s]\n",
            "\tEVAL  - Time: 00:00:20.34; AVG Loss: 0.081508; Accurancy: 0.9701; F1_maro: 0.9480; F1_weighted: 0.9699\n",
            "Exp: 1 - Fold: 1 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.106391; Accurancy: 0.9655; F1_maro: 0.9384; F1_weighted: 0.9648\n",
            "Exp: 1 - Fold: 1 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.112330; Accurancy: 0.9675; F1_maro: 0.9438; F1_weighted: 0.9674\n",
            "Test at epoch 1:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.26it/s]\n",
            "\tTEST  - Time: 00:00:10.17; AVG Loss: 0.094899; Accurancy: 0.9673; F1_maro: 0.9403; F1_weighted: 0.9674\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 2 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.100880; Accurancy: 0.9629; F1_maro: 0.9354; F1_weighted: 0.9631\n",
            "Exp: 1 - Fold: 2 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.29; AVG Loss: 0.095940; Accurancy: 0.9653; F1_maro: 0.9392; F1_weighted: 0.9654\n",
            "Exp: 1 - Fold: 2 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.095853; Accurancy: 0.9651; F1_maro: 0.9376; F1_weighted: 0.9648\n",
            "Exp: 1 - Fold: 2 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.118667; Accurancy: 0.9625; F1_maro: 0.9331; F1_weighted: 0.9622\n",
            "Exp: 1 - Fold: 2 - Epoch: 5/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.26it/s]\n",
            "\tEVAL  - Time: 00:00:20.33; AVG Loss: 0.131057; Accurancy: 0.9619; F1_maro: 0.9336; F1_weighted: 0.9621\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.30it/s]\n",
            "\tTEST  - Time: 00:00:10.15; AVG Loss: 0.071362; Accurancy: 0.9762; F1_maro: 0.9566; F1_weighted: 0.9763\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 3 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.26it/s]\n",
            "\tEVAL  - Time: 00:00:20.33; AVG Loss: 0.101593; Accurancy: 0.9653; F1_maro: 0.9383; F1_weighted: 0.9659\n",
            "Exp: 1 - Fold: 3 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.30it/s]\n",
            "\tEVAL  - Time: 00:00:20.28; AVG Loss: 0.104309; Accurancy: 0.9651; F1_maro: 0.9342; F1_weighted: 0.9647\n",
            "Exp: 1 - Fold: 3 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.118820; Accurancy: 0.9633; F1_maro: 0.9313; F1_weighted: 0.9630\n",
            "Test at epoch 1:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.26it/s]\n",
            "\tTEST  - Time: 00:00:10.17; AVG Loss: 0.086657; Accurancy: 0.9697; F1_maro: 0.9460; F1_weighted: 0.9701\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 4 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.29; AVG Loss: 0.081064; Accurancy: 0.9699; F1_maro: 0.9441; F1_weighted: 0.9698\n",
            "Exp: 1 - Fold: 4 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.25it/s]\n",
            "\tEVAL  - Time: 00:00:20.34; AVG Loss: 0.081082; Accurancy: 0.9687; F1_maro: 0.9414; F1_weighted: 0.9684\n",
            "Exp: 1 - Fold: 4 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.31it/s]\n",
            "\tEVAL  - Time: 00:00:20.27; AVG Loss: 0.079953; Accurancy: 0.9711; F1_maro: 0.9476; F1_weighted: 0.9714\n",
            "Exp: 1 - Fold: 4 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.086056; Accurancy: 0.9705; F1_maro: 0.9454; F1_weighted: 0.9705\n",
            "Exp: 1 - Fold: 4 - Epoch: 5/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.32; AVG Loss: 0.095284; Accurancy: 0.9703; F1_maro: 0.9456; F1_weighted: 0.9704\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.28it/s]\n",
            "\tTEST  - Time: 00:00:10.16; AVG Loss: 0.059569; Accurancy: 0.9806; F1_maro: 0.9649; F1_weighted: 0.9807\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 5 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.103288; Accurancy: 0.9633; F1_maro: 0.9321; F1_weighted: 0.9627\n",
            "Exp: 1 - Fold: 5 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.26it/s]\n",
            "\tEVAL  - Time: 00:00:20.33; AVG Loss: 0.105315; Accurancy: 0.9613; F1_maro: 0.9268; F1_weighted: 0.9602\n",
            "Exp: 1 - Fold: 5 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.26it/s]\n",
            "\tEVAL  - Time: 00:00:20.33; AVG Loss: 0.093885; Accurancy: 0.9671; F1_maro: 0.9416; F1_weighted: 0.9673\n",
            "Exp: 1 - Fold: 5 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.33; AVG Loss: 0.105695; Accurancy: 0.9651; F1_maro: 0.9366; F1_weighted: 0.9649\n",
            "Exp: 1 - Fold: 5 - Epoch: 5/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.30it/s]\n",
            "\tEVAL  - Time: 00:00:20.28; AVG Loss: 0.123569; Accurancy: 0.9637; F1_maro: 0.9337; F1_weighted: 0.9634\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.30it/s]\n",
            "\tTEST  - Time: 00:00:10.14; AVG Loss: 0.063404; Accurancy: 0.9794; F1_maro: 0.9628; F1_weighted: 0.9796\n",
            "Summary:\n",
            "\tAccurancy: [0.9673255344897136, 0.9762000806776926, 0.9697458652682533, 0.9806373537716822, 0.9794271883824123]\n",
            "\tAVG Accurancy: 0.9747 - MAX: 0.9806 - MIN: 0.9673\n",
            "\tF1 Macro: [0.9402955563081419, 0.9565976287024631, 0.9459945014167638, 0.9648630452499911, 0.9627764298124204]\n",
            "\tAVG F1 Macro: 0.9541 - MAX: 0.9649 - MIN: 0.9403\n",
            "\tF1 Weighted: [0.9673741495583854, 0.9762589115659737, 0.9701359424117997, 0.9807323225165447, 0.9795577431842277]\n",
            "\t AVG F1 Weighted: 0.9748 - MAX: 0.9807 - MIN: 0.9674\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 1 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.32; AVG Loss: 0.099638; Accurancy: 0.9639; F1_maro: 0.9390; F1_weighted: 0.9647\n",
            "Exp: 2 - Fold: 1 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.089830; Accurancy: 0.9671; F1_maro: 0.9405; F1_weighted: 0.9667\n",
            "Exp: 2 - Fold: 1 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.32; AVG Loss: 0.088398; Accurancy: 0.9687; F1_maro: 0.9445; F1_weighted: 0.9687\n",
            "Exp: 2 - Fold: 1 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.26it/s]\n",
            "\tEVAL  - Time: 00:00:20.33; AVG Loss: 0.091725; Accurancy: 0.9675; F1_maro: 0.9407; F1_weighted: 0.9670\n",
            "Exp: 2 - Fold: 1 - Epoch: 5/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.25it/s]\n",
            "\tEVAL  - Time: 00:00:20.34; AVG Loss: 0.105852; Accurancy: 0.9683; F1_maro: 0.9434; F1_weighted: 0.9682\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 14.97it/s]\n",
            "\tTEST  - Time: 00:00:10.37; AVG Loss: 0.041373; Accurancy: 0.9871; F1_maro: 0.9770; F1_weighted: 0.9871\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 2 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.091758; Accurancy: 0.9695; F1_maro: 0.9474; F1_weighted: 0.9698\n",
            "Exp: 2 - Fold: 2 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.104050; Accurancy: 0.9653; F1_maro: 0.9366; F1_weighted: 0.9646\n",
            "Exp: 2 - Fold: 2 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.29; AVG Loss: 0.097915; Accurancy: 0.9671; F1_maro: 0.9430; F1_weighted: 0.9673\n",
            "Test at epoch 1:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.26it/s]\n",
            "\tTEST  - Time: 00:00:10.17; AVG Loss: 0.066676; Accurancy: 0.9758; F1_maro: 0.9581; F1_weighted: 0.9761\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 3 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.088105; Accurancy: 0.9685; F1_maro: 0.9441; F1_weighted: 0.9690\n",
            "Exp: 2 - Fold: 3 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.30it/s]\n",
            "\tEVAL  - Time: 00:00:20.28; AVG Loss: 0.094894; Accurancy: 0.9683; F1_maro: 0.9443; F1_weighted: 0.9689\n",
            "Exp: 2 - Fold: 3 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.091008; Accurancy: 0.9689; F1_maro: 0.9441; F1_weighted: 0.9692\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.29it/s]\n",
            "\tTEST  - Time: 00:00:10.15; AVG Loss: 0.054460; Accurancy: 0.9814; F1_maro: 0.9678; F1_weighted: 0.9817\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 4 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.26it/s]\n",
            "\tEVAL  - Time: 00:00:20.33; AVG Loss: 0.106465; Accurancy: 0.9661; F1_maro: 0.9391; F1_weighted: 0.9658\n",
            "Exp: 2 - Fold: 4 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.098328; Accurancy: 0.9651; F1_maro: 0.9379; F1_weighted: 0.9650\n",
            "Exp: 2 - Fold: 4 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.32; AVG Loss: 0.123694; Accurancy: 0.9635; F1_maro: 0.9338; F1_weighted: 0.9630\n",
            "Exp: 2 - Fold: 4 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.119173; Accurancy: 0.9655; F1_maro: 0.9378; F1_weighted: 0.9651\n",
            "Test at epoch 1:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.26it/s]\n",
            "\tTEST  - Time: 00:00:10.17; AVG Loss: 0.071408; Accurancy: 0.9786; F1_maro: 0.9618; F1_weighted: 0.9785\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 5 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.13it/s]\n",
            "\tEVAL  - Time: 00:00:20.50; AVG Loss: 0.097221; Accurancy: 0.9645; F1_maro: 0.9341; F1_weighted: 0.9642\n",
            "Exp: 2 - Fold: 5 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.098870; Accurancy: 0.9635; F1_maro: 0.9311; F1_weighted: 0.9628\n",
            "Exp: 2 - Fold: 5 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.104854; Accurancy: 0.9677; F1_maro: 0.9426; F1_weighted: 0.9681\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.29it/s]\n",
            "\tTEST  - Time: 00:00:10.15; AVG Loss: 0.050912; Accurancy: 0.9855; F1_maro: 0.9745; F1_weighted: 0.9856\n",
            "Summary:\n",
            "\tAccurancy: [0.9870915691811214, 0.9757966922146026, 0.9814441306978621, 0.9786204114562324, 0.9854780153287616]\n",
            "\tAVG Accurancy: 0.9817 - MAX: 0.9871 - MIN: 0.9758\n",
            "\tF1 Macro: [0.977023426095419, 0.9581028246932498, 0.9678203851823758, 0.9617630324857216, 0.974489410893025]\n",
            "\tAVG F1 Macro: 0.9678 - MAX: 0.9770 - MIN: 0.9581\n",
            "\tF1 Weighted: [0.9870792984400054, 0.9761042206383133, 0.9816637322994757, 0.9785487215092131, 0.9855590621961424]\n",
            "\t AVG F1 Weighted: 0.9818 - MAX: 0.9871 - MIN: 0.9761\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 3 - Fold: 1 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.098039; Accurancy: 0.9655; F1_maro: 0.9360; F1_weighted: 0.9654\n",
            "Exp: 3 - Fold: 1 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.32; AVG Loss: 0.100969; Accurancy: 0.9643; F1_maro: 0.9330; F1_weighted: 0.9640\n",
            "Exp: 3 - Fold: 1 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.099364; Accurancy: 0.9647; F1_maro: 0.9344; F1_weighted: 0.9645\n",
            "Test at epoch 1:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.32it/s]\n",
            "\tTEST  - Time: 00:00:10.13; AVG Loss: 0.090386; Accurancy: 0.9669; F1_maro: 0.9422; F1_weighted: 0.9668\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 3 - Fold: 2 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.29; AVG Loss: 0.084678; Accurancy: 0.9705; F1_maro: 0.9501; F1_weighted: 0.9708\n",
            "Exp: 3 - Fold: 2 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.087674; Accurancy: 0.9703; F1_maro: 0.9476; F1_weighted: 0.9699\n",
            "Exp: 3 - Fold: 2 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.101431; Accurancy: 0.9703; F1_maro: 0.9504; F1_weighted: 0.9707\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.28it/s]\n",
            "\tTEST  - Time: 00:00:10.16; AVG Loss: 0.066069; Accurancy: 0.9762; F1_maro: 0.9602; F1_weighted: 0.9766\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 3 - Fold: 3 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.30it/s]\n",
            "\tEVAL  - Time: 00:00:20.29; AVG Loss: 0.087942; Accurancy: 0.9691; F1_maro: 0.9441; F1_weighted: 0.9689\n",
            "Exp: 3 - Fold: 3 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.085416; Accurancy: 0.9720; F1_maro: 0.9501; F1_weighted: 0.9720\n",
            "Exp: 3 - Fold: 3 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.31it/s]\n",
            "\tEVAL  - Time: 00:00:20.27; AVG Loss: 0.082288; Accurancy: 0.9720; F1_maro: 0.9501; F1_weighted: 0.9720\n",
            "Exp: 3 - Fold: 3 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.099055; Accurancy: 0.9699; F1_maro: 0.9466; F1_weighted: 0.9700\n",
            "Exp: 3 - Fold: 3 - Epoch: 5/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.29; AVG Loss: 0.100506; Accurancy: 0.9677; F1_maro: 0.9407; F1_weighted: 0.9673\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.26it/s]\n",
            "\tTEST  - Time: 00:00:10.17; AVG Loss: 0.054892; Accurancy: 0.9790; F1_maro: 0.9638; F1_weighted: 0.9791\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 3 - Fold: 4 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.30it/s]\n",
            "\tEVAL  - Time: 00:00:20.29; AVG Loss: 0.093413; Accurancy: 0.9671; F1_maro: 0.9411; F1_weighted: 0.9674\n",
            "Exp: 3 - Fold: 4 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.32it/s]\n",
            "\tEVAL  - Time: 00:00:20.25; AVG Loss: 0.093574; Accurancy: 0.9687; F1_maro: 0.9435; F1_weighted: 0.9689\n",
            "Exp: 3 - Fold: 4 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.097308; Accurancy: 0.9677; F1_maro: 0.9424; F1_weighted: 0.9681\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.35it/s]\n",
            "\tTEST  - Time: 00:00:10.12; AVG Loss: 0.067073; Accurancy: 0.9730; F1_maro: 0.9538; F1_weighted: 0.9732\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 3 - Fold: 5 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.105631; Accurancy: 0.9627; F1_maro: 0.9328; F1_weighted: 0.9624\n",
            "Exp: 3 - Fold: 5 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.31it/s]\n",
            "\tEVAL  - Time: 00:00:20.27; AVG Loss: 0.106627; Accurancy: 0.9635; F1_maro: 0.9354; F1_weighted: 0.9636\n",
            "Exp: 3 - Fold: 5 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.114703; Accurancy: 0.9594; F1_maro: 0.9251; F1_weighted: 0.9586\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.27it/s]\n",
            "\tTEST  - Time: 00:00:10.17; AVG Loss: 0.071600; Accurancy: 0.9750; F1_maro: 0.9571; F1_weighted: 0.9751\n",
            "Summary:\n",
            "\tAccurancy: [0.9669221460266236, 0.9762000806776926, 0.9790237999193223, 0.972972972972973, 0.9749899152884227]\n",
            "\tAVG Accurancy: 0.9740 - MAX: 0.9790 - MIN: 0.9669\n",
            "\tF1 Macro: [0.9422144522144522, 0.9601930403888459, 0.9638202948533143, 0.9537582022792237, 0.9570944437000313]\n",
            "\tAVG F1 Macro: 0.9554 - MAX: 0.9638 - MIN: 0.9422\n",
            "\tF1 Weighted: [0.9668002079942377, 0.97661756709744, 0.9790805647173285, 0.9731533369031395, 0.975124047492708]\n",
            "\t AVG F1 Weighted: 0.9742 - MAX: 0.9791 - MIN: 0.9668\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 4 - Fold: 1 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.086388; Accurancy: 0.9697; F1_maro: 0.9478; F1_weighted: 0.9702\n",
            "Exp: 4 - Fold: 1 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.26it/s]\n",
            "\tEVAL  - Time: 00:00:20.33; AVG Loss: 0.085298; Accurancy: 0.9701; F1_maro: 0.9482; F1_weighted: 0.9705\n",
            "Exp: 4 - Fold: 1 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.33; AVG Loss: 0.085116; Accurancy: 0.9710; F1_maro: 0.9475; F1_weighted: 0.9708\n",
            "Exp: 4 - Fold: 1 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.33; AVG Loss: 0.118008; Accurancy: 0.9613; F1_maro: 0.9268; F1_weighted: 0.9601\n",
            "Exp: 4 - Fold: 1 - Epoch: 5/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.31it/s]\n",
            "\tEVAL  - Time: 00:00:20.27; AVG Loss: 0.112939; Accurancy: 0.9669; F1_maro: 0.9399; F1_weighted: 0.9666\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.24it/s]\n",
            "\tTEST  - Time: 00:00:10.19; AVG Loss: 0.072095; Accurancy: 0.9750; F1_maro: 0.9565; F1_weighted: 0.9751\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 4 - Fold: 2 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.094552; Accurancy: 0.9679; F1_maro: 0.9417; F1_weighted: 0.9680\n",
            "Exp: 4 - Fold: 2 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.30it/s]\n",
            "\tEVAL  - Time: 00:00:20.29; AVG Loss: 0.088736; Accurancy: 0.9691; F1_maro: 0.9444; F1_weighted: 0.9693\n",
            "Exp: 4 - Fold: 2 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.32; AVG Loss: 0.097552; Accurancy: 0.9679; F1_maro: 0.9406; F1_weighted: 0.9677\n",
            "Exp: 4 - Fold: 2 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.128105; Accurancy: 0.9661; F1_maro: 0.9356; F1_weighted: 0.9654\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.31it/s]\n",
            "\tTEST  - Time: 00:00:10.14; AVG Loss: 0.074436; Accurancy: 0.9738; F1_maro: 0.9545; F1_weighted: 0.9740\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 4 - Fold: 3 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.103883; Accurancy: 0.9661; F1_maro: 0.9404; F1_weighted: 0.9660\n",
            "Exp: 4 - Fold: 3 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.32; AVG Loss: 0.102022; Accurancy: 0.9631; F1_maro: 0.9338; F1_weighted: 0.9625\n",
            "Exp: 4 - Fold: 3 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.32; AVG Loss: 0.098635; Accurancy: 0.9671; F1_maro: 0.9433; F1_weighted: 0.9673\n",
            "Exp: 4 - Fold: 3 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.101095; Accurancy: 0.9665; F1_maro: 0.9420; F1_weighted: 0.9666\n",
            "Exp: 4 - Fold: 3 - Epoch: 5/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.33; AVG Loss: 0.131260; Accurancy: 0.9617; F1_maro: 0.9307; F1_weighted: 0.9609\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.28it/s]\n",
            "\tTEST  - Time: 00:00:10.16; AVG Loss: 0.066667; Accurancy: 0.9782; F1_maro: 0.9617; F1_weighted: 0.9782\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 4 - Fold: 4 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.105192; Accurancy: 0.9635; F1_maro: 0.9386; F1_weighted: 0.9644\n",
            "Exp: 4 - Fold: 4 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.30; AVG Loss: 0.095104; Accurancy: 0.9635; F1_maro: 0.9344; F1_weighted: 0.9632\n",
            "Exp: 4 - Fold: 4 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.29it/s]\n",
            "\tEVAL  - Time: 00:00:20.29; AVG Loss: 0.102861; Accurancy: 0.9639; F1_maro: 0.9365; F1_weighted: 0.9640\n",
            "Exp: 4 - Fold: 4 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.31; AVG Loss: 0.110961; Accurancy: 0.9613; F1_maro: 0.9299; F1_weighted: 0.9608\n",
            "Test at epoch 1:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.26it/s]\n",
            "\tTEST  - Time: 00:00:10.17; AVG Loss: 0.094741; Accurancy: 0.9677; F1_maro: 0.9454; F1_weighted: 0.9683\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 4 - Fold: 5 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:20.33; AVG Loss: 0.090641; Accurancy: 0.9677; F1_maro: 0.9412; F1_weighted: 0.9678\n",
            "Exp: 4 - Fold: 5 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.26it/s]\n",
            "\tEVAL  - Time: 00:00:20.34; AVG Loss: 0.086926; Accurancy: 0.9701; F1_maro: 0.9471; F1_weighted: 0.9706\n",
            "Exp: 4 - Fold: 5 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.28it/s]\n",
            "\tEVAL  - Time: 00:00:20.32; AVG Loss: 0.091962; Accurancy: 0.9703; F1_maro: 0.9474; F1_weighted: 0.9708\n",
            "Exp: 4 - Fold: 5 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.25it/s]\n",
            "\tEVAL  - Time: 00:00:20.34; AVG Loss: 0.101159; Accurancy: 0.9671; F1_maro: 0.9405; F1_weighted: 0.9673\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.30it/s]\n",
            "\tTEST  - Time: 00:00:10.15; AVG Loss: 0.057998; Accurancy: 0.9814; F1_maro: 0.9677; F1_weighted: 0.9815\n",
            "Summary:\n",
            "\tAccurancy: [0.9749899152884227, 0.9737797498991528, 0.9782170229931424, 0.9677289229528035, 0.9814441306978621]\n",
            "\tAVG Accurancy: 0.9752 - MAX: 0.9814 - MIN: 0.9677\n",
            "\tF1 Macro: [0.9565483137020753, 0.9544873596398193, 0.9617341163395375, 0.9454147510084729, 0.9677026310542343]\n",
            "\tAVG F1 Macro: 0.9572 - MAX: 0.9677 - MIN: 0.9454\n",
            "\tF1 Weighted: [0.9751497805653321, 0.9739590472063957, 0.9782372847099876, 0.9683483699973149, 0.9815461084317664]\n",
            "\t AVG F1 Weighted: 0.9754 - MAX: 0.9815 - MIN: 0.9683\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 5 - Fold: 1 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:58<00:00,  2.60it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.30it/s]\n",
            "\tEVAL  - Time: 00:00:20.29; AVG Loss: 0.087189; Accurancy: 0.9661; F1_maro: 0.9400; F1_weighted: 0.9661\n",
            "Exp: 5 - Fold: 1 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.38it/s]\n",
            "\tEVAL  - Time: 00:00:20.18; AVG Loss: 0.084734; Accurancy: 0.9697; F1_maro: 0.9474; F1_weighted: 0.9700\n",
            "Exp: 5 - Fold: 1 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.40it/s]\n",
            "\tEVAL  - Time: 00:00:20.15; AVG Loss: 0.092615; Accurancy: 0.9695; F1_maro: 0.9464; F1_weighted: 0.9697\n",
            "Exp: 5 - Fold: 1 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.40it/s]\n",
            "\tEVAL  - Time: 00:00:20.14; AVG Loss: 0.145342; Accurancy: 0.9560; F1_maro: 0.9164; F1_weighted: 0.9544\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.43it/s]\n",
            "\tTEST  - Time: 00:00:10.06; AVG Loss: 0.045582; Accurancy: 0.9839; F1_maro: 0.9699; F1_weighted: 0.9840\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 5 - Fold: 2 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.41it/s]\n",
            "\tEVAL  - Time: 00:00:20.14; AVG Loss: 0.092774; Accurancy: 0.9679; F1_maro: 0.9416; F1_weighted: 0.9682\n",
            "Exp: 5 - Fold: 2 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.39it/s]\n",
            "\tEVAL  - Time: 00:00:20.17; AVG Loss: 0.090756; Accurancy: 0.9685; F1_maro: 0.9416; F1_weighted: 0.9685\n",
            "Exp: 5 - Fold: 2 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.39it/s]\n",
            "\tEVAL  - Time: 00:00:20.17; AVG Loss: 0.108252; Accurancy: 0.9669; F1_maro: 0.9389; F1_weighted: 0.9670\n",
            "Exp: 5 - Fold: 2 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.39it/s]\n",
            "\tEVAL  - Time: 00:00:20.17; AVG Loss: 0.112820; Accurancy: 0.9663; F1_maro: 0.9385; F1_weighted: 0.9666\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.40it/s]\n",
            "\tTEST  - Time: 00:00:10.08; AVG Loss: 0.057379; Accurancy: 0.9827; F1_maro: 0.9671; F1_weighted: 0.9826\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 5 - Fold: 3 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.40it/s]\n",
            "\tEVAL  - Time: 00:00:20.15; AVG Loss: 0.099906; Accurancy: 0.9637; F1_maro: 0.9314; F1_weighted: 0.9632\n",
            "Exp: 5 - Fold: 3 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.39it/s]\n",
            "\tEVAL  - Time: 00:00:20.16; AVG Loss: 0.096923; Accurancy: 0.9651; F1_maro: 0.9377; F1_weighted: 0.9656\n",
            "Exp: 5 - Fold: 3 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.38it/s]\n",
            "\tEVAL  - Time: 00:00:20.17; AVG Loss: 0.109439; Accurancy: 0.9665; F1_maro: 0.9385; F1_weighted: 0.9665\n",
            "Exp: 5 - Fold: 3 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.40it/s]\n",
            "\tEVAL  - Time: 00:00:20.15; AVG Loss: 0.121042; Accurancy: 0.9643; F1_maro: 0.9364; F1_weighted: 0.9648\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.39it/s]\n",
            "\tTEST  - Time: 00:00:10.09; AVG Loss: 0.042118; Accurancy: 0.9875; F1_maro: 0.9764; F1_weighted: 0.9875\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 5 - Fold: 4 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.38it/s]\n",
            "\tEVAL  - Time: 00:00:20.18; AVG Loss: 0.078570; Accurancy: 0.9730; F1_maro: 0.9523; F1_weighted: 0.9729\n",
            "Exp: 5 - Fold: 4 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.38it/s]\n",
            "\tEVAL  - Time: 00:00:20.19; AVG Loss: 0.078454; Accurancy: 0.9728; F1_maro: 0.9520; F1_weighted: 0.9727\n",
            "Exp: 5 - Fold: 4 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.41it/s]\n",
            "\tEVAL  - Time: 00:00:20.14; AVG Loss: 0.084386; Accurancy: 0.9722; F1_maro: 0.9517; F1_weighted: 0.9724\n",
            "Exp: 5 - Fold: 4 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.38it/s]\n",
            "\tEVAL  - Time: 00:00:20.18; AVG Loss: 0.089123; Accurancy: 0.9711; F1_maro: 0.9490; F1_weighted: 0.9711\n",
            "Test at epoch 1:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.39it/s]\n",
            "\tTEST  - Time: 00:00:10.09; AVG Loss: 0.065296; Accurancy: 0.9790; F1_maro: 0.9605; F1_weighted: 0.9791\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 5 - Fold: 5 - Epoch: 1/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.37it/s]\n",
            "\tEVAL  - Time: 00:00:20.18; AVG Loss: 0.107387; Accurancy: 0.9635; F1_maro: 0.9366; F1_weighted: 0.9634\n",
            "Exp: 5 - Fold: 5 - Epoch: 2/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.38it/s]\n",
            "\tEVAL  - Time: 00:00:20.18; AVG Loss: 0.101890; Accurancy: 0.9661; F1_maro: 0.9421; F1_weighted: 0.9663\n",
            "Exp: 5 - Fold: 5 - Epoch: 3/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.36it/s]\n",
            "\tEVAL  - Time: 00:00:20.20; AVG Loss: 0.123486; Accurancy: 0.9600; F1_maro: 0.9283; F1_weighted: 0.9592\n",
            "Exp: 5 - Fold: 5 - Epoch: 4/10\n",
            "\tTRAIN:: 100%|██████████| 620/620 [03:57<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 310/310 [00:20<00:00, 15.39it/s]\n",
            "\tEVAL  - Time: 00:00:20.17; AVG Loss: 0.112758; Accurancy: 0.9637; F1_maro: 0.9358; F1_weighted: 0.9632\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 155/155 [00:10<00:00, 15.37it/s]\n",
            "\tTEST  - Time: 00:00:10.10; AVG Loss: 0.057387; Accurancy: 0.9802; F1_maro: 0.9631; F1_weighted: 0.9804\n",
            "Summary:\n",
            "\tAccurancy: [0.9838644614764018, 0.982654296087132, 0.9874949576442114, 0.9790237999193223, 0.9802339653085922]\n",
            "\tAVG Accurancy: 0.9827 - MAX: 0.9875 - MIN: 0.9790\n",
            "\tF1 Macro: [0.9698850798124348, 0.9671182127351106, 0.9764428519448465, 0.9605258645187211, 0.9631470190349374]\n",
            "\tAVG F1 Macro: 0.9674 - MAX: 0.9764 - MIN: 0.9605\n",
            "\tF1 Weighted: [0.9839637821276556, 0.9826269438277094, 0.9875144843327263, 0.9790674014170255, 0.9803655590891358]\n",
            "\t AVG F1 Weighted: 0.9827 - MAX: 0.9875 - MIN: 0.9791\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRsHnVELnKqf"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnpFDrDAnQrT",
        "outputId": "61ebe276-c203-4b70-fdb7-72f74cfd45d5"
      },
      "source": [
        "logger.info(f\"Accurancy: {exp_scores['acc']}\")\n",
        "logger.info(f\"AVG Accurancy: {(sum(exp_scores['acc'])/num_exp):.4f} - MAX: {max(exp_scores['acc']):.4f} - MIN: {min(exp_scores['acc']):.4f}\")\n",
        "logger.info(f\"F1 Macro: {exp_scores['f1_macro']}\")\n",
        "logger.info(f\"AVG F1 Macro: {(sum(exp_scores['f1_macro'])/num_exp):.4f} - MAX: {max(exp_scores['f1_macro']):.4f} - MIN: {min(exp_scores['f1_macro']):.4f}\")\n",
        "logger.info(f\"F1 Weighted: {exp_scores['f1_weighted']}\")\n",
        "logger.info(f\"AVG F1 Weighted: {(sum(exp_scores['f1_weighted'])/num_exp):.4f} - MAX: {max(exp_scores['f1_weighted']):.4f} - MIN: {min(exp_scores['f1_weighted']):.4f}\")\n",
        "logger.info(f\"Exp time: {exp_scores['exp_time']}\")\n",
        "logger.info(f\"AVG Exp time: {parse_time(sum(exp_scores['exp_time'])/num_exp)}\")\n",
        "logger.info(f\"Fold time: {exp_scores['fold_time']}\")\n",
        "logger.info(f\"AVG Fold time: {parse_time(sum(exp_scores['fold_time'])/len(exp_scores['fold_time']))}\")\n",
        "logger.info(f\"Epoch time: {exp_scores['epoch_time']}\")\n",
        "logger.info(f\"AVG Epoch time: {parse_time(sum(exp_scores['epoch_time'])/len(exp_scores['epoch_time']))}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accurancy: [0.9746672045179506, 0.981686163775716, 0.9740217829770069, 0.9752319483662767, 0.9826542960871321]\n",
            "AVG Accurancy: 0.9777 - MAX: 0.9827 - MIN: 0.9740\n",
            "F1 Macro: [0.9541054322979561, 0.9678398158699583, 0.9554160866871735, 0.957177434348828, 0.96742380560921]\n",
            "AVG F1 Macro: 0.9604 - MAX: 0.9678 - MIN: 0.9541\n",
            "F1 Weighted: [0.9748118138473864, 0.98179100701663, 0.9741551448409707, 0.9754481181821593, 0.9827076341588505]\n",
            "AVG F1 Weighted: 0.9778 - MAX: 0.9827 - MIN: 0.9742\n",
            "Exp time: [5235.850959062576, 4282.668512105942, 4047.149932384491, 5238.381111383438, 4755.714826822281]\n",
            "AVG Exp time: 01:18:31.95\n",
            "Fold time: [951.7350206375122, 1189.7493391036987, 713.9705059528351, 1190.251636981964, 1190.1444563865662, 1189.5103492736816, 713.7171165943146, 713.7561626434326, 951.6523752212524, 714.0325083732605, 714.0691299438477, 714.1224255561829, 1190.0536375045776, 714.4440062046051, 714.4607331752777, 1190.303902387619, 952.2989089488983, 1190.3329434394836, 952.4688613414764, 952.9764952659607, 951.5491833686829, 950.9788081645966, 950.6165707111359, 951.1830439567566, 951.387220621109]\n",
            "AVG Fold time: 00:15:42.39\n",
            "Epoch time: [238.0995740890503, 237.90006351470947, 237.84929895401, 237.88608407974243, 237.85693073272705, 238.2044563293457, 237.8759458065033, 237.93248510360718, 237.8795211315155, 237.88840413093567, 238.13333010673523, 237.94877171516418, 237.9643006324768, 238.04295444488525, 238.0706124305725, 237.97059035301208, 238.20317912101746, 237.9840521812439, 238.03519558906555, 238.01223611831665, 237.94170427322388, 238.1712682247162, 237.90829944610596, 237.9002423286438, 237.94247794151306, 237.9214630126953, 237.8378665447235, 237.8595359325409, 237.93199181556702, 237.92558884620667, 237.8107249736786, 237.85879349708557, 238.08664417266846, 237.92221999168396, 237.86576199531555, 237.93240427970886, 237.93198895454407, 237.9916217327118, 238.02854895591736, 238.01233768463135, 237.96840453147888, 238.03248596191406, 238.0682394504547, 238.16171503067017, 237.96749186515808, 237.99321866035461, 237.92987608909607, 238.0276780128479, 238.13556838035583, 238.00524950027466, 237.95526552200317, 238.14698219299316, 238.10861897468567, 238.18840503692627, 238.3173224925995, 238.07131958007812, 238.0720911026001, 238.0961046218872, 238.11767077445984, 238.03444170951843, 238.0358202457428, 238.01986503601074, 238.01479482650757, 238.0075798034668, 238.28606605529785, 237.9904682636261, 237.96665906906128, 238.03928971290588, 238.058660030365, 237.9910089969635, 238.277325630188, 238.0459976196289, 238.06309413909912, 238.24476599693298, 238.11500358581543, 238.36056542396545, 238.2084608078003, 238.24442052841187, 238.16304850578308, 238.29104852676392, 237.7902021408081, 237.72073030471802, 237.74720239639282, 237.70746755599976, 237.65173721313477, 237.84526181221008, 237.77434158325195, 237.71567487716675, 237.68062663078308, 237.64653420448303, 237.573734998703, 237.7074110507965, 237.83335399627686, 237.91232109069824, 237.72995781898499, 237.71682500839233, 237.93102192878723, 237.79360508918762, 237.94576859474182]\n",
            "AVG Epoch time: 00:03:57.98\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efNZ4f7wpjGd"
      },
      "source": [
        "## HS2-2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30iAltsZpjGe"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMCdBhz5pjGe"
      },
      "source": [
        "task_name = \"HS2_2021\"\n",
        "model_name = \"vinai/bertweet-base\"\n",
        "# Dataset Options\n",
        "train_file_path = \"./datasets/HS2-2021/hs2_2021.csv\"\n",
        "output_path = \"./outputs\"\n",
        "text_idx = 0\n",
        "class_idx = 1\n",
        "delimiter = \",\"\n",
        "label_maps = {\"0\": 0, \"1\": 1}\n",
        "test_split = 0.1\n",
        "# Task Options\n",
        "num_exp = 5\n",
        "kfold = 5\n",
        "num_epochs = 5\n",
        "train_batch_size = 32\n",
        "test_batch_size = 16\n",
        "max_length = 128\n",
        "weight_decay = 0.0\n",
        "learning_rate = 0.00001\n",
        "early_stop = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ib0xJMopjGe"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2i6aSAGpjGe",
        "outputId": "8f4862d5-c157-44f8-f503-3eb587a2f9e0"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, normalization=True)\n",
        "train_dataset = BertweetDataset(train_file_path, tokenizer, label_maps, text_idx, class_idx, delimiter, batch_size=train_batch_size, max_length=max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "100%|██████████| 31788/31788 [00:09<00:00, 3367.50it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBWx0QY2pjGe",
        "outputId": "e9859d60-d2c3-4be0-afca-c1a625fa837f"
      },
      "source": [
        "exp_scores = {\"acc\": [], \"f1_macro\": [], \"f1_weighted\": [], \"exp_time\": [], \"epoch_time\": [], \"fold_time\": []}\n",
        "for exp in range(num_exp):\n",
        "    exp_time = 0\n",
        "    kf = KFold(n_splits=kfold, shuffle=True)\n",
        "    avg_scores = {\"acc\": [], \"f1_macro\": [], \"f1_weighted\": []}\n",
        "    data_idxs = list(range(train_dataset.__len__()))\n",
        "    train_idx, test_idxs = train_test_split(data_idxs, test_size=test_split, shuffle=True)\n",
        "    test_iter = DataLoader(train_dataset, batch_size=test_batch_size, sampler=SubsetRandomSampler(test_idxs))\n",
        "    for idx, (train_idx, eval_idx) in enumerate(kf.split(data_idxs)):\n",
        "        best_epoch, best_loss, best_score = 0, float(\"inf\"), 0\n",
        "        model, optimizer = build_model(model_name, len(set(label_maps.values())), task_name, learning_rate, weight_decay)\n",
        "        best_model = copy.deepcopy(model)\n",
        "        train_iter = DataLoader(train_dataset, batch_size=train_batch_size, sampler=SubsetRandomSampler(train_idx))\n",
        "        eval_iter = DataLoader(train_dataset, batch_size=test_batch_size, sampler=SubsetRandomSampler(eval_idx))\n",
        "        fold_time = 0\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_time = time.time()\n",
        "            logger.info(f\"Exp: {exp+1} - Fold: {idx+1} - Epoch: {epoch+1}/{num_epochs}\")\n",
        "            model, train_loss, train_score, train_time = train(model, optimizer, train_iter)\n",
        "            epoch_time = time.time() - epoch_time\n",
        "            fold_time += epoch_time\n",
        "            exp_scores[\"epoch_time\"].append(epoch_time)\n",
        "            eval_loss, eval_score, eval_time = eval(model, eval_iter)\n",
        "            logger.info(f\"\\tEVAL  - Time: {eval_time}; AVG Loss: {eval_loss:.6f}; Accurancy: {eval_score[0]:.4f}; F1_maro: {eval_score[1]:.4f}; F1_weighted: {eval_score[2]:.4f}\")\n",
        "            if best_score <= eval_score[1]:\n",
        "              best_model = copy.deepcopy(model)\n",
        "              best_score = eval_score[1]\n",
        "              best_epoch = epoch\n",
        "            if best_loss >= eval_loss:\n",
        "              best_loss = eval_loss\n",
        "              counter = 0\n",
        "            else:\n",
        "              counter += 1\n",
        "            if counter >= early_stop:\n",
        "              break\n",
        "        exp_time += fold_time\n",
        "        exp_scores[\"fold_time\"].append(fold_time)\n",
        "        logger.info(f\"Test at epoch {best_epoch+1}:\")\n",
        "        test_loss, test_score, test_time = eval(best_model, test_iter)\n",
        "        logger.info(f\"\\tTEST  - Time: {test_time}; AVG Loss: {test_loss:.6f}; Accurancy: {test_score[0]:.4f}; F1_maro: {test_score[1]:.4f}; F1_weighted: {test_score[2]:.4f}\")\n",
        "        avg_scores[\"acc\"].append(test_score[0])\n",
        "        avg_scores[\"f1_macro\"].append(test_score[1])\n",
        "        avg_scores[\"f1_weighted\"].append(test_score[2])\n",
        "    logger.info(\"Summary:\")\n",
        "    fold_acc = (sum(avg_scores['acc'])/kfold)\n",
        "    fold_f1_macro = (sum(avg_scores['f1_macro'])/kfold)\n",
        "    fold_f1_weighted = (sum(avg_scores['f1_weighted'])/kfold)\n",
        "    logger.info(f\"\\tAccurancy: {avg_scores['acc']}\")\n",
        "    logger.info(f\"\\tAVG Accurancy: {fold_acc:.4f} - MAX: {max(avg_scores['acc']):.4f} - MIN: {min(avg_scores['acc']):.4f}\")\n",
        "    logger.info(f\"\\tF1 Macro: {avg_scores['f1_macro']}\")\n",
        "    logger.info(f\"\\tAVG F1 Macro: {fold_f1_macro:.4f} - MAX: {max(avg_scores['f1_macro']):.4f} - MIN: {min(avg_scores['f1_macro']):.4f}\")\n",
        "    logger.info(f\"\\tF1 Weighted: {avg_scores['f1_weighted']}\")\n",
        "    logger.info(f\"\\t AVG F1 Weighted: {(sum(avg_scores['f1_weighted'])/num_exp):.4f} - MAX: {max(avg_scores['f1_weighted']):.4f} - MIN: {min(avg_scores['f1_weighted']):.4f}\")\n",
        "    exp_scores[\"acc\"].append(fold_acc)\n",
        "    exp_scores[\"f1_macro\"].append(fold_f1_macro)\n",
        "    exp_scores[\"f1_weighted\"].append(fold_f1_weighted)\n",
        "    exp_scores[\"exp_time\"].append(exp_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 1 - Epoch: 1/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.41it/s]\n",
            "\tEVAL  - Time: 00:00:25.85; AVG Loss: 0.175996; Accurancy: 0.9199; F1_maro: 0.9012; F1_weighted: 0.9204\n",
            "Exp: 1 - Fold: 1 - Epoch: 2/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:26<00:00, 15.27it/s]\n",
            "\tEVAL  - Time: 00:00:26.08; AVG Loss: 0.181893; Accurancy: 0.9141; F1_maro: 0.8905; F1_weighted: 0.9132\n",
            "Exp: 1 - Fold: 1 - Epoch: 3/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.39it/s]\n",
            "\tEVAL  - Time: 00:00:25.88; AVG Loss: 0.185924; Accurancy: 0.9243; F1_maro: 0.9077; F1_weighted: 0.9252\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 199/199 [00:12<00:00, 15.40it/s]\n",
            "\tTEST  - Time: 00:00:12.94; AVG Loss: 0.127346; Accurancy: 0.9421; F1_maro: 0.9292; F1_weighted: 0.9430\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 2 - Epoch: 1/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.42it/s]\n",
            "\tEVAL  - Time: 00:00:25.84; AVG Loss: 0.203377; Accurancy: 0.9083; F1_maro: 0.8900; F1_weighted: 0.9109\n",
            "Exp: 1 - Fold: 2 - Epoch: 2/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.39it/s]\n",
            "\tEVAL  - Time: 00:00:25.89; AVG Loss: 0.198217; Accurancy: 0.9042; F1_maro: 0.8751; F1_weighted: 0.9032\n",
            "Exp: 1 - Fold: 2 - Epoch: 3/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.42it/s]\n",
            "\tEVAL  - Time: 00:00:25.83; AVG Loss: 0.220003; Accurancy: 0.9089; F1_maro: 0.8900; F1_weighted: 0.9113\n",
            "Exp: 1 - Fold: 2 - Epoch: 4/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.41it/s]\n",
            "\tEVAL  - Time: 00:00:25.85; AVG Loss: 0.229444; Accurancy: 0.9063; F1_maro: 0.8798; F1_weighted: 0.9061\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 199/199 [00:12<00:00, 15.40it/s]\n",
            "\tTEST  - Time: 00:00:12.93; AVG Loss: 0.132516; Accurancy: 0.9402; F1_maro: 0.9281; F1_weighted: 0.9415\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 3 - Epoch: 1/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.40it/s]\n",
            "\tEVAL  - Time: 00:00:25.87; AVG Loss: 0.204526; Accurancy: 0.9066; F1_maro: 0.8890; F1_weighted: 0.9097\n",
            "Exp: 1 - Fold: 3 - Epoch: 2/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.40it/s]\n",
            "\tEVAL  - Time: 00:00:25.87; AVG Loss: 0.198334; Accurancy: 0.9146; F1_maro: 0.8945; F1_weighted: 0.9160\n",
            "Exp: 1 - Fold: 3 - Epoch: 3/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.40it/s]\n",
            "\tEVAL  - Time: 00:00:25.86; AVG Loss: 0.195043; Accurancy: 0.9140; F1_maro: 0.8945; F1_weighted: 0.9157\n",
            "Exp: 1 - Fold: 3 - Epoch: 4/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.43it/s]\n",
            "\tEVAL  - Time: 00:00:25.81; AVG Loss: 0.224633; Accurancy: 0.9126; F1_maro: 0.8887; F1_weighted: 0.9128\n",
            "Exp: 1 - Fold: 3 - Epoch: 5/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.42it/s]\n",
            "\tEVAL  - Time: 00:00:25.84; AVG Loss: 0.254814; Accurancy: 0.9004; F1_maro: 0.8686; F1_weighted: 0.8988\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 199/199 [00:12<00:00, 15.43it/s]\n",
            "\tTEST  - Time: 00:00:12.91; AVG Loss: 0.148130; Accurancy: 0.9346; F1_maro: 0.9200; F1_weighted: 0.9355\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 4 - Epoch: 1/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.43it/s]\n",
            "\tEVAL  - Time: 00:00:25.82; AVG Loss: 0.205192; Accurancy: 0.9110; F1_maro: 0.8927; F1_weighted: 0.9130\n",
            "Exp: 1 - Fold: 4 - Epoch: 2/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.43it/s]\n",
            "\tEVAL  - Time: 00:00:25.82; AVG Loss: 0.590935; Accurancy: 0.7329; F1_maro: 0.4443; F1_weighted: 0.6281\n",
            "Exp: 1 - Fold: 4 - Epoch: 3/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.44it/s]\n",
            "\tEVAL  - Time: 00:00:25.80; AVG Loss: 0.593440; Accurancy: 0.7294; F1_maro: 0.4218; F1_weighted: 0.6153\n",
            "Test at epoch 1:\n",
            "\tEVAL:: 100%|██████████| 199/199 [00:12<00:00, 15.42it/s]\n",
            "\tTEST  - Time: 00:00:12.92; AVG Loss: 0.176627; Accurancy: 0.9210; F1_maro: 0.9049; F1_weighted: 0.9227\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 1 - Fold: 5 - Epoch: 1/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.43it/s]\n",
            "\tEVAL  - Time: 00:00:25.82; AVG Loss: 0.203192; Accurancy: 0.9160; F1_maro: 0.8962; F1_weighted: 0.9166\n",
            "Exp: 1 - Fold: 5 - Epoch: 2/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.35it/s]\n",
            "\tEVAL  - Time: 00:00:25.95; AVG Loss: 0.184172; Accurancy: 0.9215; F1_maro: 0.9038; F1_weighted: 0.9223\n",
            "Exp: 1 - Fold: 5 - Epoch: 3/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.40it/s]\n",
            "\tEVAL  - Time: 00:00:25.88; AVG Loss: 0.205092; Accurancy: 0.9127; F1_maro: 0.8886; F1_weighted: 0.9119\n",
            "Exp: 1 - Fold: 5 - Epoch: 4/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:05<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.35it/s]\n",
            "\tEVAL  - Time: 00:00:25.94; AVG Loss: 0.235773; Accurancy: 0.9086; F1_maro: 0.8830; F1_weighted: 0.9076\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 199/199 [00:12<00:00, 15.41it/s]\n",
            "\tTEST  - Time: 00:00:12.93; AVG Loss: 0.142129; Accurancy: 0.9343; F1_maro: 0.9182; F1_weighted: 0.9347\n",
            "Summary:\n",
            "\tAccurancy: [0.9421201635734507, 0.9402327776030198, 0.934570619691727, 0.9210443535703051, 0.9342560553633218]\n",
            "\tAVG Accurancy: 0.9344 - MAX: 0.9421 - MIN: 0.9210\n",
            "\tF1 Macro: [0.9291773013236213, 0.9280751416562829, 0.9199921786822628, 0.9048945636208885, 0.91821201423954]\n",
            "\tAVG F1 Macro: 0.9201 - MAX: 0.9292 - MIN: 0.9049\n",
            "\tF1 Weighted: [0.9429582591422404, 0.941535049242303, 0.9355375022936812, 0.922733301068832, 0.9347004570705857]\n",
            "\t AVG F1 Weighted: 0.9355 - MAX: 0.9430 - MIN: 0.9227\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 1 - Epoch: 1/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.38it/s]\n",
            "\tEVAL  - Time: 00:00:25.90; AVG Loss: 0.198771; Accurancy: 0.9070; F1_maro: 0.8820; F1_weighted: 0.9065\n",
            "Exp: 2 - Fold: 1 - Epoch: 2/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.36it/s]\n",
            "\tEVAL  - Time: 00:00:25.94; AVG Loss: 0.193974; Accurancy: 0.9155; F1_maro: 0.8994; F1_weighted: 0.9176\n",
            "Exp: 2 - Fold: 1 - Epoch: 3/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.36it/s]\n",
            "\tEVAL  - Time: 00:00:25.94; AVG Loss: 0.196544; Accurancy: 0.9144; F1_maro: 0.8928; F1_weighted: 0.9145\n",
            "Exp: 2 - Fold: 1 - Epoch: 4/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:05<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.36it/s]\n",
            "\tEVAL  - Time: 00:00:25.93; AVG Loss: 0.241061; Accurancy: 0.9055; F1_maro: 0.8784; F1_weighted: 0.9043\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 199/199 [00:12<00:00, 15.36it/s]\n",
            "\tTEST  - Time: 00:00:12.97; AVG Loss: 0.153960; Accurancy: 0.9302; F1_maro: 0.9158; F1_weighted: 0.9319\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 2 - Epoch: 1/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.35it/s]\n",
            "\tEVAL  - Time: 00:00:25.94; AVG Loss: 0.186384; Accurancy: 0.9182; F1_maro: 0.8998; F1_weighted: 0.9195\n",
            "Exp: 2 - Fold: 2 - Epoch: 2/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.35it/s]\n",
            "\tEVAL  - Time: 00:00:25.95; AVG Loss: 0.193247; Accurancy: 0.9133; F1_maro: 0.8969; F1_weighted: 0.9158\n",
            "Exp: 2 - Fold: 2 - Epoch: 3/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.34it/s]\n",
            "\tEVAL  - Time: 00:00:25.97; AVG Loss: 0.189667; Accurancy: 0.9187; F1_maro: 0.9008; F1_weighted: 0.9201\n",
            "Test at epoch 3:\n",
            "\tEVAL:: 100%|██████████| 199/199 [00:12<00:00, 15.35it/s]\n",
            "\tTEST  - Time: 00:00:12.98; AVG Loss: 0.121808; Accurancy: 0.9446; F1_maro: 0.9319; F1_weighted: 0.9456\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 3 - Epoch: 1/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:05<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.32it/s]\n",
            "\tEVAL  - Time: 00:00:26.00; AVG Loss: 0.189859; Accurancy: 0.9111; F1_maro: 0.8929; F1_weighted: 0.9133\n",
            "Exp: 2 - Fold: 3 - Epoch: 2/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.35it/s]\n",
            "\tEVAL  - Time: 00:00:25.95; AVG Loss: 0.183438; Accurancy: 0.9190; F1_maro: 0.9009; F1_weighted: 0.9204\n",
            "Exp: 2 - Fold: 3 - Epoch: 3/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.33it/s]\n",
            "\tEVAL  - Time: 00:00:25.99; AVG Loss: 0.188072; Accurancy: 0.9100; F1_maro: 0.8865; F1_weighted: 0.9103\n",
            "Exp: 2 - Fold: 3 - Epoch: 4/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.34it/s]\n",
            "\tEVAL  - Time: 00:00:25.96; AVG Loss: 0.227260; Accurancy: 0.9072; F1_maro: 0.8782; F1_weighted: 0.9056\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 199/199 [00:12<00:00, 15.33it/s]\n",
            "\tTEST  - Time: 00:00:12.99; AVG Loss: 0.151683; Accurancy: 0.9336; F1_maro: 0.9185; F1_weighted: 0.9348\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 4 - Epoch: 1/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.36it/s]\n",
            "\tEVAL  - Time: 00:00:25.93; AVG Loss: 0.202391; Accurancy: 0.9100; F1_maro: 0.8863; F1_weighted: 0.9106\n",
            "Exp: 2 - Fold: 4 - Epoch: 2/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.34it/s]\n",
            "\tEVAL  - Time: 00:00:25.96; AVG Loss: 0.208640; Accurancy: 0.9132; F1_maro: 0.8950; F1_weighted: 0.9154\n",
            "Exp: 2 - Fold: 4 - Epoch: 3/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.35it/s]\n",
            "\tEVAL  - Time: 00:00:25.95; AVG Loss: 0.208216; Accurancy: 0.9141; F1_maro: 0.8932; F1_weighted: 0.9153\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 199/199 [00:12<00:00, 15.35it/s]\n",
            "\tTEST  - Time: 00:00:12.97; AVG Loss: 0.157087; Accurancy: 0.9251; F1_maro: 0.9099; F1_weighted: 0.9271\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 2 - Fold: 5 - Epoch: 1/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.34it/s]\n",
            "\tEVAL  - Time: 00:00:25.98; AVG Loss: 0.193612; Accurancy: 0.9165; F1_maro: 0.8961; F1_weighted: 0.9168\n",
            "Exp: 2 - Fold: 5 - Epoch: 2/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.36it/s]\n",
            "\tEVAL  - Time: 00:00:25.94; AVG Loss: 0.189471; Accurancy: 0.9210; F1_maro: 0.9045; F1_weighted: 0.9224\n",
            "Exp: 2 - Fold: 5 - Epoch: 3/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.36it/s]\n",
            "\tEVAL  - Time: 00:00:25.94; AVG Loss: 0.208747; Accurancy: 0.9106; F1_maro: 0.8853; F1_weighted: 0.9095\n",
            "Exp: 2 - Fold: 5 - Epoch: 4/5\n",
            "\tTRAIN:: 100%|██████████| 795/795 [05:04<00:00,  2.61it/s]\n",
            "\tEVAL:: 100%|██████████| 398/398 [00:25<00:00, 15.37it/s]\n",
            "\tEVAL  - Time: 00:00:25.92; AVG Loss: 0.215552; Accurancy: 0.9146; F1_maro: 0.8931; F1_weighted: 0.9146\n",
            "Test at epoch 2:\n",
            "\tEVAL:: 100%|██████████| 199/199 [00:12<00:00, 15.38it/s]\n",
            "\tTEST  - Time: 00:00:12.95; AVG Loss: 0.149763; Accurancy: 0.9311; F1_maro: 0.9161; F1_weighted: 0.9325\n",
            "Summary:\n",
            "\tAccurancy: [0.9301667190940547, 0.9446366782006921, 0.9336269267065115, 0.9251336898395722, 0.9311104120792703]\n",
            "\tAVG Accurancy: 0.9329 - MAX: 0.9446 - MIN: 0.9251\n",
            "\tF1 Macro: [0.9158038728371696, 0.931941370981711, 0.9184885761466476, 0.9099046511273439, 0.9160629707661565]\n",
            "\tAVG F1 Macro: 0.9184 - MAX: 0.9319 - MIN: 0.9099\n",
            "\tF1 Weighted: [0.9319169516919995, 0.9455613182020745, 0.934765065471231, 0.9270679059269706, 0.9325301929461095]\n",
            "\t AVG F1 Weighted: 0.9344 - MAX: 0.9456 - MIN: 0.9271\n",
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Exp: 3 - Fold: 1 - Epoch: 1/5\n",
            "\tTRAIN::  68%|██████▊   | 544/795 [03:28<01:36,  2.61it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qjea-FWpjGe"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRHw74yNpjGe"
      },
      "source": [
        "logger.info(f\"Accurancy: {exp_scores['acc']}\")\n",
        "logger.info(f\"AVG Accurancy: {(sum(exp_scores['acc'])/num_exp):.4f} - MAX: {max(exp_scores['acc']):.4f} - MIN: {min(exp_scores['acc']):.4f}\")\n",
        "logger.info(f\"F1 Macro: {exp_scores['f1_macro']}\")\n",
        "logger.info(f\"AVG F1 Macro: {(sum(exp_scores['f1_macro'])/num_exp):.4f} - MAX: {max(exp_scores['f1_macro']):.4f} - MIN: {min(exp_scores['f1_macro']):.4f}\")\n",
        "logger.info(f\"F1 Weighted: {exp_scores['f1_weighted']}\")\n",
        "logger.info(f\"AVG F1 Weighted: {(sum(exp_scores['f1_weighted'])/num_exp):.4f} - MAX: {max(exp_scores['f1_weighted']):.4f} - MIN: {min(exp_scores['f1_weighted']):.4f}\")\n",
        "logger.info(f\"Exp time: {exp_scores['exp_time']}\")\n",
        "logger.info(f\"AVG Exp time: {parse_time(sum(exp_scores['exp_time'])/num_exp)}\")\n",
        "logger.info(f\"Fold time: {exp_scores['fold_time']}\")\n",
        "logger.info(f\"AVG Fold time: {parse_time(sum(exp_scores['fold_time'])/len(exp_scores['fold_time']))}\")\n",
        "logger.info(f\"Epoch time: {exp_scores['epoch_time']}\")\n",
        "logger.info(f\"AVG Epoch time: {parse_time(sum(exp_scores['epoch_time'])/len(exp_scores['epoch_time']))}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}